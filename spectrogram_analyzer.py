# -*- coding: utf-8 -*-
"""
é¢‘è°±å›¾åˆ†ææ¨¡å— - é¢‘è°±é•œå­æ ¸å¿ƒåŠŸèƒ½
ç”¨äºå¯è§†åŒ–å‘éŸ³ç‰¹å¾ï¼Œå¸®åŠ©ç”¨æˆ·åŒºåˆ†é€æ°”/ä¸é€æ°”éŸ³ï¼ˆå¦‚ zhi/chiï¼‰
"""
import librosa
import numpy as np
import matplotlib
matplotlib.use('Agg')  # éGUIåç«¯
import matplotlib.pyplot as plt
from scipy import signal
import os
import json
from typing import Tuple, Dict, Optional, List
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SpectrogramAnalyzer:
    """é¢‘è°±å›¾åˆ†æå™¨"""
    
    # é»˜è®¤å‚æ•°
    SAMPLE_RATE = 16000      # é‡‡æ ·ç‡
    N_FFT = 512              # FFTçª—å£å¤§å°
    HOP_LENGTH = 128         # å¸§ç§»
    F_MIN = 80               # æœ€ä½é¢‘ç‡
    F_MAX = 8000             # æœ€é«˜é¢‘ç‡
    
    def __init__(self, sample_rate=16000):
        self.sample_rate = sample_rate
        logger.info(f"é¢‘è°±åˆ†æå™¨åˆå§‹åŒ–å®Œæˆï¼Œé‡‡æ ·ç‡: {sample_rate}Hz")
    
    def generate_spectrogram(self, audio_path: str, output_image_path: Optional[str] = None) -> Dict:
        """
        ç”Ÿæˆå®Œæ•´çš„é¢‘è°±å›¾
        
        å‚æ•°:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            output_image_path: å¯é€‰ï¼Œä¿å­˜å›¾åƒçš„è·¯å¾„
        
        è¿”å›:
            åŒ…å«é¢‘è°±å›¾æ•°æ®çš„å­—å…¸
        """
        try:
            # åŠ è½½éŸ³é¢‘
            y, sr = librosa.load(audio_path, sr=self.sample_rate)
            logger.info(f"åŠ è½½éŸ³é¢‘: {audio_path}, æ—¶é•¿: {len(y)/sr:.2f}ç§’")
            
            # ç”ŸæˆSTFTï¼ˆçŸ­æ—¶å‚…é‡Œå¶å˜æ¢ï¼‰
            D = librosa.stft(y, n_fft=self.N_FFT, hop_length=self.HOP_LENGTH)
            
            # è½¬æ¢ä¸ºåˆ†è´åˆ»åº¦
            S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)
            
            # æ—¶é—´å’Œé¢‘ç‡è½´
            times = librosa.frames_to_time(
                range(S_db.shape[1]), 
                sr=sr, 
                hop_length=self.HOP_LENGTH
            )
            frequencies = librosa.fft_frequencies(sr=sr, n_fft=self.N_FFT)
            
            # å¯è§†åŒ–å¹¶ä¿å­˜
            if output_image_path:
                self._save_spectrogram_image(S_db, sr, output_image_path)
            
            return {
                'success': True,
                'spectrogram': S_db.tolist(),
                'times': times.tolist(),
                'frequencies': frequencies.tolist(),
                'duration': float(len(y) / sr),
                'sample_rate': sr
            }
        
        except Exception as e:
            logger.error(f"ç”Ÿæˆé¢‘è°±å›¾å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _save_spectrogram_image(self, S_db, sr, output_path):
        """ä¿å­˜é¢‘è°±å›¾ä¸ºå›¾ç‰‡"""
        plt.figure(figsize=(12, 6))
        librosa.display.specshow(
            S_db, 
            sr=sr, 
            hop_length=self.HOP_LENGTH,
            x_axis='time', 
            y_axis='hz',
            cmap='hot'
        )
        plt.colorbar(format='%+2.0f dB')
        plt.title('è¯­éŸ³é¢‘è°±å›¾', fontsize=16, fontproperties='SimHei')
        plt.xlabel('æ—¶é—´ (ç§’)', fontproperties='SimHei')
        plt.ylabel('é¢‘ç‡ (Hz)', fontproperties='SimHei')
        plt.ylim([0, 8000])
        plt.tight_layout()
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        logger.info(f"é¢‘è°±å›¾å·²ä¿å­˜: {output_path}")
    
    def detect_vot(self, audio_path: str, threshold_db: float = -40, 
                   min_duration_ms: float = 20) -> Dict:
        """
        è‡ªåŠ¨æ£€æµ‹VOTï¼ˆVoice Onset Timeï¼Œé€æ°”æŒç»­æ—¶é—´ï¼‰
        
        å‚æ•°:
            audio_path: éŸ³é¢‘è·¯å¾„
            threshold_db: èƒ½é‡é˜ˆå€¼ï¼ˆdBï¼‰
            min_duration_ms: æœ€å°æŒç»­æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
        
        è¿”å›:
            åŒ…å«VOTæ£€æµ‹ç»“æœçš„å­—å…¸
        """
        try:
            y, sr = librosa.load(audio_path, sr=self.sample_rate)
            
            # è®¡ç®—çŸ­æ—¶èƒ½é‡
            frame_length = int(0.01 * sr)  # 10mså¸§
            hop_length_energy = frame_length // 2
            
            energy = np.array([
                np.sum(y[i:i+frame_length]**2) 
                for i in range(0, len(y)-frame_length, hop_length_energy)
            ])
            
            # è½¬æ¢ä¸ºdB
            energy_db = 10 * np.log10(energy + 1e-10)
            
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªè¶…è¿‡é˜ˆå€¼çš„ç‚¹ï¼ˆçˆ†ç ´å¼€å§‹ï¼‰
            burst_frames = np.where(energy_db > threshold_db)[0]
            if len(burst_frames) == 0:
                return {
                    'success': False,
                    'error': 'æœªæ£€æµ‹åˆ°æœ‰æ•ˆéŸ³é¢‘ä¿¡å·'
                }
            
            burst_start = burst_frames[0]
            
            # æ‰¾åˆ°èƒ½é‡ç¨³å®šå¢é•¿çš„ç‚¹ï¼ˆæµŠéŸ³å¼€å§‹ï¼‰
            peak_energy = np.max(energy_db[burst_start:burst_start+100]) if len(energy_db) > burst_start+100 else np.max(energy_db[burst_start:])
            voice_threshold = peak_energy - 10
            voice_frames = np.where(energy_db[burst_start:] > voice_threshold)[0]
            
            if len(voice_frames) == 0:
                return {
                    'success': False,
                    'error': 'æœªæ£€æµ‹åˆ°æµŠéŸ³å¼€å§‹ç‚¹'
                }
            
            voice_start = burst_start + voice_frames[0]
            
            # è®¡ç®—VOTï¼ˆæ¯«ç§’ï¼‰
            vot_frames = voice_start - burst_start
            vot_ms = vot_frames * hop_length_energy / sr * 1000
            
            # è®¡ç®—æ—¶é—´ç‚¹
            burst_time = burst_start * hop_length_energy / sr
            voice_time = voice_start * hop_length_energy / sr
            
            return {
                'success': True,
                'vot_ms': float(vot_ms),
                'burst_start_time': float(burst_time),
                'voice_start_time': float(voice_time),
                'burst_start_frame': int(burst_start),
                'voice_start_frame': int(voice_start)
            }
        
        except Exception as e:
            logger.error(f"VOTæ£€æµ‹å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def quantify_aspiration(self, audio_path: str, vot_region: Optional[Tuple[int, int]] = None) -> Dict:
        """
        é‡åŒ–é€æ°”æ®µçš„å¼ºåº¦å’Œç‰¹å¾
        
        å‚æ•°:
            audio_path: éŸ³é¢‘è·¯å¾„
            vot_region: å¯é€‰ï¼ŒVOTåŒºåŸŸçš„æ ·æœ¬ç´¢å¼•èŒƒå›´ (start, end)
        
        è¿”å›:
            é€æ°”å¼ºåº¦è¯„åˆ†å’Œè¯¦ç»†ç‰¹å¾
        """
        try:
            y, sr = librosa.load(audio_path, sr=self.sample_rate)
            
            # å¦‚æœæ²¡æœ‰æŒ‡å®šVOTåŒºåŸŸï¼Œè‡ªåŠ¨æ£€æµ‹
            if vot_region is None:
                vot_result = self.detect_vot(audio_path)
                if not vot_result['success']:
                    return vot_result
                
                # è½¬æ¢ä¸ºæ ·æœ¬ç´¢å¼•
                frame_samples = int(0.01 * sr)
                hop = frame_samples // 2
                start_sample = int(vot_result['burst_start_frame'] * hop)
                end_sample = int(vot_result['voice_start_frame'] * hop)
            else:
                start_sample, end_sample = vot_region
            
            # æå–é€æ°”æ®µ
            aspiration_segment = y[start_sample:end_sample]
            
            if len(aspiration_segment) < 100:  # å¤ªçŸ­
                return {
                    'success': False,
                    'error': 'é€æ°”æ®µè¿‡çŸ­ï¼Œæ— æ³•åˆ†æ'
                }
            
            # ç‰¹å¾1ï¼šæŒç»­æ—¶é—´
            duration_ms = len(aspiration_segment) / sr * 1000
            
            # ç‰¹å¾2ï¼šå¹³å‡èƒ½é‡
            energy = np.mean(aspiration_segment**2)
            energy_db = 10 * np.log10(energy + 1e-10)
            
            # ç‰¹å¾3ï¼šé«˜é¢‘èƒ½é‡å æ¯”
            D = librosa.stft(aspiration_segment, n_fft=512)
            power_spectrum = np.abs(D)**2
            freqs = librosa.fft_frequencies(sr=sr, n_fft=512)
            
            high_freq_mask = freqs > 3000
            high_freq_ratio = (np.sum(power_spectrum[high_freq_mask, :]) / 
                              (np.sum(power_spectrum) + 1e-10))
            
            # ç‰¹å¾4ï¼šé¢‘è°±å¹³å¦åº¦ï¼ˆå™ªéŸ³ç‰¹å¾ï¼‰
            spectral_flatness = np.mean(librosa.feature.spectral_flatness(y=aspiration_segment))
            
            # ç»¼åˆè¯„åˆ†ï¼ˆç»éªŒå…¬å¼ï¼Œ0-100åˆ†ï¼‰
            duration_score = min(duration_ms / 100 * 100, 100)
            energy_score = min((energy_db + 60) / 30 * 100, 100)
            high_freq_score = high_freq_ratio * 100
            flatness_score = float(spectral_flatness) * 100
            
            aspiration_score = (
                duration_score * 0.4 + 
                energy_score * 0.3 + 
                high_freq_score * 0.2 + 
                flatness_score * 0.1
            )
            
            return {
                'success': True,
                'aspiration_score': float(aspiration_score),
                'duration_ms': float(duration_ms),
                'energy_db': float(energy_db),
                'high_freq_ratio': float(high_freq_ratio),
                'spectral_flatness': float(spectral_flatness),
                'duration_score': float(duration_score),
                'energy_score': float(energy_score),
                'high_freq_score': float(high_freq_score)
            }
        
        except Exception as e:
            logger.error(f"é€æ°”å¼ºåº¦é‡åŒ–å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def classify_zhi_chi(self, audio_path: str) -> Dict:
        """
        è‡ªåŠ¨åˆ†ç±» zhiï¼ˆä¸é€æ°”ï¼‰vs chiï¼ˆé€æ°”ï¼‰
        
        å‚æ•°:
            audio_path: éŸ³é¢‘è·¯å¾„
        
        è¿”å›:
            åˆ†ç±»ç»“æœå’Œè¯¦ç»†åˆ†æ
        """
        try:
            # æ£€æµ‹VOT
            vot_result = self.detect_vot(audio_path)
            if not vot_result['success']:
                return vot_result
            
            vot_ms = vot_result['vot_ms']
            
            # é‡åŒ–é€æ°”å¼ºåº¦
            aspiration_result = self.quantify_aspiration(audio_path)
            if not aspiration_result['success']:
                return aspiration_result
            
            aspiration_score = aspiration_result['aspiration_score']
            high_freq_ratio = aspiration_result['high_freq_ratio']
            
            # åˆ†ç±»è§„åˆ™ï¼ˆåŸºäºæ ‡å‡†æ™®é€šè¯æ•°æ®ï¼‰
            zhi_score = 0
            chi_score = 0
            
            # VOTç»´åº¦ï¼ˆæœ€é‡è¦ï¼‰
            if vot_ms < 30:
                zhi_score += 3
            elif vot_ms > 60:
                chi_score += 3
            else:
                ratio = (vot_ms - 30) / 30
                chi_score += ratio * 2
                zhi_score += (1 - ratio) * 2
            
            # é€æ°”å¼ºåº¦ç»´åº¦
            if aspiration_score < 40:
                zhi_score += 2
            elif aspiration_score > 60:
                chi_score += 2
            else:
                ratio = (aspiration_score - 40) / 20
                chi_score += ratio
                zhi_score += (1 - ratio)
            
            # é«˜é¢‘èƒ½é‡ç»´åº¦
            if high_freq_ratio > 0.3:
                chi_score += 1
            else:
                zhi_score += 1
            
            # å½’ä¸€åŒ–ç½®ä¿¡åº¦
            total = zhi_score + chi_score
            zhi_confidence = zhi_score / total
            chi_confidence = chi_score / total
            
            if chi_confidence > zhi_confidence:
                prediction = 'chi'
                confidence = chi_confidence
            else:
                prediction = 'zhi'
                confidence = zhi_confidence
            
            # ç”Ÿæˆåé¦ˆå»ºè®®
            feedback = self._generate_feedback(
                prediction, 
                vot_ms, 
                aspiration_score,
                chi_confidence,
                zhi_confidence
            )
            
            return {
                'success': True,
                'prediction': prediction,
                'confidence': float(confidence),
                'vot_ms': float(vot_ms),
                'aspiration_score': float(aspiration_score),
                'zhi_confidence': float(zhi_confidence),
                'chi_confidence': float(chi_confidence),
                'feedback': feedback
            }
        
        except Exception as e:
            logger.error(f"zhi/chiåˆ†ç±»å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _generate_feedback(self, prediction: str, vot_ms: float, 
                          aspiration_score: float, chi_conf: float, zhi_conf: float) -> str:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        feedback = []
        
        if prediction == 'chi':
            feedback.append(f"âœ“ è¯†åˆ«ä¸ºã€é€æ°”éŸ³ chiã€‘ï¼Œç½®ä¿¡åº¦ {chi_conf*100:.1f}%")
            if vot_ms < 60:
                feedback.append("ğŸ’¡ é€æ°”æ—¶é•¿åçŸ­ï¼Œå°è¯•æ›´ç”¨åŠ›åœ°å¹æ°”")
            if aspiration_score < 60:
                feedback.append("ğŸ’¡ é€æ°”å¼ºåº¦å¯ä»¥å†å¼ºä¸€äº›")
        else:
            feedback.append(f"âœ“ è¯†åˆ«ä¸ºã€ä¸é€æ°”éŸ³ zhiã€‘ï¼Œç½®ä¿¡åº¦ {zhi_conf*100:.1f}%")
            if vot_ms > 30:
                feedback.append("ğŸ’¡ æœ‰è½»å¾®é€æ°”ï¼Œå°è¯•æ›´è½»æŸ”åœ°å‘éŸ³")
            if aspiration_score > 40:
                feedback.append("ğŸ’¡ æ”¾æ¾ï¼Œä¸è¦å¹æ°”")
        
        # å¦‚æœä¸¤è€…æ¥è¿‘ï¼Œç»™å‡ºæç¤º
        if abs(chi_conf - zhi_conf) < 0.2:
            feedback.append("âš ï¸ å‘éŸ³ç‰¹å¾ä¸å¤Ÿæ˜æ˜¾ï¼Œè¯·åŠ å¼ºå¯¹æ¯”")
        
        return " | ".join(feedback)
    
    def analyze_audio(self, audio_path: str, target_phoneme: Optional[str] = None) -> Dict:
        """
        å®Œæ•´éŸ³é¢‘åˆ†æï¼ˆé¢‘è°±é•œå­æ ¸å¿ƒå‡½æ•°ï¼‰
        
        å‚æ•°:
            audio_path: éŸ³é¢‘è·¯å¾„
            target_phoneme: ç›®æ ‡éŸ³ç´ ï¼ˆ'zhi'æˆ–'chi'ï¼‰ï¼Œç”¨äºè¯„åˆ†
        
        è¿”å›:
            å®Œæ•´çš„åˆ†æç»“æœ
        """
        try:
            # ç”Ÿæˆé¢‘è°±å›¾æ•°æ®
            spec_result = self.generate_spectrogram(audio_path)
            if not spec_result['success']:
                return spec_result
            
            # åˆ†ç±»
            classify_result = self.classify_zhi_chi(audio_path)
            if not classify_result['success']:
                return classify_result
            
            # å¦‚æœæŒ‡å®šäº†ç›®æ ‡éŸ³ç´ ï¼Œè®¡ç®—å‡†ç¡®æ€§è¯„åˆ†
            score = None
            grade = None
            if target_phoneme:
                score = self._calculate_score(
                    classify_result['prediction'],
                    target_phoneme,
                    classify_result['vot_ms'],
                    classify_result['aspiration_score']
                )
                grade = self._get_grade(score)
            
            return {
                'success': True,
                'spectrogram_data': {
                    'spec': spec_result['spectrogram'],
                    'times': spec_result['times'],
                    'frequencies': spec_result['frequencies']
                },
                'classification': {
                    'prediction': classify_result['prediction'],
                    'confidence': classify_result['confidence'],
                    'zhi_confidence': classify_result['zhi_confidence'],
                    'chi_confidence': classify_result['chi_confidence']
                },
                'features': {
                    'vot_ms': classify_result['vot_ms'],
                    'aspiration_score': classify_result['aspiration_score']
                },
                'feedback': classify_result['feedback'],
                'score': score,
                'grade': grade,
                'target_phoneme': target_phoneme
            }
        
        except Exception as e:
            logger.error(f"å®Œæ•´åˆ†æå¤±è´¥: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _calculate_score(self, prediction: str, target: str, 
                        vot_ms: float, aspiration_score: float) -> float:
        """è®¡ç®—è¯„åˆ†ï¼ˆ0-100ï¼‰"""
        score = 0
        
        # åˆ†ç±»æ­£ç¡®æ€§ï¼ˆ40åˆ†ï¼‰
        if prediction == target:
            score += 40
        
        # VOTå‡†ç¡®æ€§ï¼ˆ30åˆ†ï¼‰
        target_vot = 20 if target == 'zhi' else 80
        vot_error = abs(vot_ms - target_vot)
        score += max(0, 30 - vot_error / 2)
        
        # é€æ°”å¼ºåº¦ï¼ˆ30åˆ†ï¼‰
        target_aspiration = 30 if target == 'zhi' else 75
        asp_error = abs(aspiration_score - target_aspiration)
        score += max(0, 30 - asp_error / 5)
        
        return min(100, max(0, score))
    
    def _get_grade(self, score: float) -> str:
        """è·å–ç­‰çº§"""
        if score >= 90:
            return 'S'
        elif score >= 80:
            return 'A'
        elif score >= 70:
            return 'B'
        elif score >= 60:
            return 'C'
        else:
            return 'D'


# å…¨å±€å®ä¾‹
_analyzer_instance = None

def get_analyzer(sample_rate=16000) -> SpectrogramAnalyzer:
    """è·å–é¢‘è°±åˆ†æå™¨å•ä¾‹"""
    global _analyzer_instance
    if _analyzer_instance is None:
        _analyzer_instance = SpectrogramAnalyzer(sample_rate)
    return _analyzer_instance


if __name__ == '__main__':
    # æµ‹è¯•ä»£ç 
    analyzer = SpectrogramAnalyzer()
    test_audio = 'test_audio.wav'
    
    if os.path.exists(test_audio):
        print("=" * 50)
        print("æµ‹è¯•é¢‘è°±å›¾ç”Ÿæˆ...")
        result = analyzer.generate_spectrogram(test_audio, 'test_spec.png')
        print(json.dumps(result, indent=2, ensure_ascii=False))
        
        print("\n" + "=" * 50)
        print("æµ‹è¯•å®Œæ•´åˆ†æ...")
        analysis = analyzer.analyze_audio(test_audio, target_phoneme='chi')
        print(json.dumps(analysis, indent=2, ensure_ascii=False))
    else:
        print(f"æµ‹è¯•éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {test_audio}")

