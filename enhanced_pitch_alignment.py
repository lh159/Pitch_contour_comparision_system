#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºçš„éŸ³é«˜å¯¹é½æ¨¡å—
è§£å†³éŸ³é«˜æ›²çº¿æ¯”å¯¹ä¸­çš„å…³é”®é—®é¢˜ï¼š
1. æˆªæ–­éŸ³é«˜æ›²çº¿åˆ°TTSç»“æŸæ—¶é—´
2. ä½¿ç”¨ASRå¯¹é½ç”¨æˆ·å‘éŸ³ä¸TTSæ ‡å‡†å‘éŸ³çš„æ—¶é—´è½´  
3. éªŒè¯ç”¨æˆ·çœŸå®å½•éŸ³è¾“å…¥ï¼Œå¤„ç†é™éŸ³å’Œå‡éŸ³é¢‘é—®é¢˜
"""

import numpy as np
import parselmouth
import librosa
import soundfile as sf
from typing import Dict, List, Tuple, Optional
import os
import tempfile
from scipy.interpolate import interp1d
from config import Config

try:
    from vad_module import VADProcessor
    VAD_AVAILABLE = True
except ImportError:
    VAD_AVAILABLE = False
    print("è­¦å‘Š: VADæ¨¡å—ä¸å¯ç”¨")

try:
    from fun_asr_module import FunASRProcessor
    FUN_ASR_AVAILABLE = True
except ImportError:
    FUN_ASR_AVAILABLE = False
    print("è­¦å‘Š: Fun-ASRæ¨¡å—ä¸å¯ç”¨")


class EnhancedPitchAligner:
    """å¢å¼ºçš„éŸ³é«˜å¯¹é½å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¢å¼ºçš„éŸ³é«˜å¯¹é½å™¨"""
        # åˆå§‹åŒ–VADå¤„ç†å™¨
        self.vad_processor = None
        if VAD_AVAILABLE:
            try:
                self.vad_processor = VADProcessor()
                print("âœ“ VADå¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ VADå¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # åˆå§‹åŒ–Fun-ASRå¤„ç†å™¨
        self.fun_asr_processor = None
        if FUN_ASR_AVAILABLE:
            try:
                self.fun_asr_processor = FunASRProcessor()
                print("âœ“ Fun-ASRå¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ Fun-ASRå¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # éŸ³é¢‘è´¨é‡æ£€æµ‹é˜ˆå€¼
        self.silence_energy_threshold = 0.005  # é™éŸ³èƒ½é‡é˜ˆå€¼
        self.min_speech_ratio = 0.1  # æœ€å°è¯­éŸ³æ¯”ä¾‹
        self.min_pitch_validity = 0.15  # æœ€å°æœ‰æ•ˆéŸ³é«˜æ¯”ä¾‹
    
    def get_tts_audio_duration(self, tts_audio_path: str, text: str = None) -> float:
        """
        è·å–TTSéŸ³é¢‘çš„å®é™…æœ‰æ•ˆæ—¶é•¿ï¼ˆæ’é™¤å°¾éƒ¨é™éŸ³ï¼‰
        :param tts_audio_path: TTSéŸ³é¢‘æ–‡ä»¶è·¯å¾„
        :param text: åŸå§‹æ–‡æœ¬ï¼ˆç”¨äºè¾…åŠ©éªŒè¯ï¼‰
        :return: æœ‰æ•ˆéŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
        """
        try:
            print(f"ğŸ” åˆ†æTTSéŸ³é¢‘æ—¶é•¿: {tts_audio_path}")
            
            # æ–¹æ³•1: ä½¿ç”¨VADæ£€æµ‹è¯­éŸ³ç»“æŸæ—¶é—´
            if self.vad_processor:
                vad_info = self.vad_processor.get_speech_regions_timestamps(tts_audio_path)
                speech_segments = vad_info.get('speech_segments', [])
                
                if speech_segments:
                    # æ‰¾åˆ°æœ€åä¸€ä¸ªè¯­éŸ³æ®µçš„ç»“æŸæ—¶é—´
                    last_speech_end = max(end for start, end in speech_segments)
                    print(f"âœ“ VADæ£€æµ‹åˆ°çš„è¯­éŸ³ç»“æŸæ—¶é—´: {last_speech_end:.3f}s")
                    return last_speech_end
            
            # æ–¹æ³•2: ä½¿ç”¨èƒ½é‡æ£€æµ‹
            y, sr = librosa.load(tts_audio_path, sr=None)
            
            # è®¡ç®—çŸ­æ—¶èƒ½é‡
            frame_length = int(0.025 * sr)  # 25ms
            hop_length = int(0.010 * sr)    # 10ms
            
            rms = librosa.feature.rms(
                y=y, 
                frame_length=frame_length, 
                hop_length=hop_length
            )[0]
            
            # åŠ¨æ€é˜ˆå€¼ï¼šå¹³å‡èƒ½é‡çš„30%
            energy_threshold = np.mean(rms) * 0.3
            
            # ä»åå‘å‰æŸ¥æ‰¾æœ€åçš„æœ‰æ•ˆè¯­éŸ³
            times = librosa.frames_to_time(np.arange(len(rms)), sr=sr, hop_length=hop_length)
            
            for i in range(len(rms) - 1, -1, -1):
                if rms[i] > energy_threshold:
                    # æ‰¾åˆ°æœ€åçš„æœ‰æ•ˆè¯­éŸ³å¸§ï¼Œå†å»¶é•¿ä¸€ç‚¹ç‚¹
                    effective_duration = times[i] + 0.1  # å¢åŠ 100msç¼“å†²
                    print(f"âœ“ èƒ½é‡æ£€æµ‹åˆ°çš„è¯­éŸ³ç»“æŸæ—¶é—´: {effective_duration:.3f}s")
                    return min(effective_duration, len(y) / sr)  # ä¸è¶…è¿‡æ€»æ—¶é•¿
            
            # æ–¹æ³•3: å¦‚æœéƒ½å¤±è´¥ï¼Œä½¿ç”¨æ€»æ—¶é•¿çš„90%ä½œä¸ºä¿å®ˆä¼°è®¡
            total_duration = len(y) / sr
            conservative_duration = total_duration * 0.9
            print(f"âš ï¸ ä½¿ç”¨ä¿å®ˆä¼°è®¡çš„è¯­éŸ³ç»“æŸæ—¶é—´: {conservative_duration:.3f}s")
            return conservative_duration
            
        except Exception as e:
            print(f"âŒ è·å–TTSéŸ³é¢‘æ—¶é•¿å¤±è´¥: {e}")
            # æœ€åçš„å…œåº•æ–¹æ¡ˆï¼šå°è¯•ç›´æ¥è·å–éŸ³é¢‘æ—¶é•¿
            try:
                sound = parselmouth.Sound(tts_audio_path)
                return sound.duration * 0.9  # å–90%ä½œä¸ºä¿å®ˆä¼°è®¡
            except:
                return 3.0  # é»˜è®¤3ç§’
    
    def validate_user_audio_quality(self, user_audio_path: str) -> Dict:
        """
        éªŒè¯ç”¨æˆ·å½•éŸ³çš„è´¨é‡ï¼Œæ£€æµ‹æ˜¯å¦ä¸ºçœŸå®å½•éŸ³
        :param user_audio_path: ç”¨æˆ·éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        :return: éªŒè¯ç»“æœ
        """
        try:
            print(f"ğŸ¯ éªŒè¯ç”¨æˆ·å½•éŸ³è´¨é‡: {user_audio_path}")
            
            # åŠ è½½éŸ³é¢‘
            y, sr = librosa.load(user_audio_path, sr=None)
            total_duration = len(y) / sr
            
            # æ£€æŸ¥1: éŸ³é¢‘é•¿åº¦æ˜¯å¦åˆç†
            if total_duration < 0.3:
                return {
                    'is_valid': False,
                    'reason': 'å½•éŸ³æ—¶é—´è¿‡çŸ­',
                    'details': f'å½•éŸ³æ—¶é•¿ä»…{total_duration:.2f}sï¼Œå¯èƒ½æœªæ­£ç¡®å½•éŸ³'
                }
            
            # æ£€æŸ¥2: è®¡ç®—RMSèƒ½é‡
            rms = np.sqrt(np.mean(y**2))
            if rms < self.silence_energy_threshold:
                return {
                    'is_valid': False,
                    'reason': 'å½•éŸ³éŸ³é‡è¿‡å°æˆ–ä¸ºé™éŸ³',
                    'details': f'RMSèƒ½é‡{rms:.6f}ä½äºé˜ˆå€¼{self.silence_energy_threshold}'
                }
            
            # æ£€æŸ¥3: ä½¿ç”¨VADæ£€æµ‹å®é™…è¯­éŸ³æ¯”ä¾‹
            speech_ratio = 0.0
            speech_segments = []
            
            if self.vad_processor:
                vad_info = self.vad_processor.get_speech_regions_timestamps(user_audio_path)
                speech_ratio = vad_info.get('speech_ratio', 0.0)
                speech_segments = vad_info.get('speech_segments', [])
            else:
                # ç®€å•çš„èƒ½é‡æ£€æµ‹
                frame_length = int(0.025 * sr)
                hop_length = int(0.010 * sr)
                rms_frames = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]
                
                # åŠ¨æ€é˜ˆå€¼
                energy_threshold = np.mean(rms_frames) * 0.3
                speech_frames = rms_frames > energy_threshold
                speech_ratio = np.mean(speech_frames)
            
            if speech_ratio < self.min_speech_ratio:
                return {
                    'is_valid': False,
                    'reason': 'è¯­éŸ³å†…å®¹æ¯”ä¾‹è¿‡ä½',
                    'details': f'è¯­éŸ³æ¯”ä¾‹{speech_ratio:.2%}ä½äº{self.min_speech_ratio:.2%}ï¼Œå¯èƒ½å½•éŸ³æœ‰é—®é¢˜'
                }
            
            # æ£€æŸ¥4: éŸ³é«˜æ£€æµ‹éªŒè¯
            try:
                sound = parselmouth.Sound(user_audio_path)
                pitch = sound.to_pitch()
                pitch_values = pitch.selected_array['frequency']
                pitch_values = pitch_values[pitch_values > 0]  # åªè€ƒè™‘æœ‰æ•ˆéŸ³é«˜
                
                if len(pitch_values) == 0:
                    return {
                        'is_valid': False,
                        'reason': 'æœªæ£€æµ‹åˆ°æœ‰æ•ˆéŸ³é«˜',
                        'details': 'æ— æ³•ä»å½•éŸ³ä¸­æå–éŸ³é«˜ä¿¡æ¯ï¼Œå¯èƒ½ä¸ºçº¯å™ªéŸ³æˆ–é™éŸ³'
                    }
                
                valid_pitch_ratio = len(pitch_values) / len(pitch.selected_array['frequency'])
                if valid_pitch_ratio < self.min_pitch_validity:
                    return {
                        'is_valid': False,
                        'reason': 'æœ‰æ•ˆéŸ³é«˜æ¯”ä¾‹è¿‡ä½',
                        'details': f'æœ‰æ•ˆéŸ³é«˜æ¯”ä¾‹{valid_pitch_ratio:.2%}ä½äº{self.min_pitch_validity:.2%}'
                    }
                
            except Exception as e:
                print(f"âš ï¸ éŸ³é«˜æ£€æµ‹å¤±è´¥: {e}")
                # éŸ³é«˜æ£€æµ‹å¤±è´¥ä¸ä¸€å®šæ„å‘³ç€å½•éŸ³æ— æ•ˆï¼Œç»§ç»­å…¶ä»–æ£€æŸ¥
            
            # æ£€æŸ¥5: éŸ³é¢‘åŠ¨æ€èŒƒå›´
            amplitude_range = np.max(np.abs(y)) - np.min(np.abs(y))
            if amplitude_range < 0.01:  # åŠ¨æ€èŒƒå›´è¿‡å°
                return {
                    'is_valid': False,
                    'reason': 'éŸ³é¢‘åŠ¨æ€èŒƒå›´è¿‡å°',
                    'details': f'å¯èƒ½ä¸ºå•è°ƒéŸ³é¢‘æˆ–æŸåæ–‡ä»¶ï¼ŒåŠ¨æ€èŒƒå›´{amplitude_range:.4f}'
                }
            
            # æ‰€æœ‰æ£€æŸ¥é€šè¿‡
            return {
                'is_valid': True,
                'reason': 'å½•éŸ³è´¨é‡è‰¯å¥½',
                'details': {
                    'duration': total_duration,
                    'rms_energy': rms,
                    'speech_ratio': speech_ratio,
                    'speech_segments_count': len(speech_segments),
                    'amplitude_range': amplitude_range
                }
            }
            
        except Exception as e:
            print(f"âŒ ç”¨æˆ·å½•éŸ³è´¨é‡éªŒè¯å¤±è´¥: {e}")
            return {
                'is_valid': False,
                'reason': 'å½•éŸ³æ–‡ä»¶åˆ†æå¤±è´¥',
                'details': f'æ— æ³•åˆ†æå½•éŸ³æ–‡ä»¶: {str(e)}'
            }
    
    def align_user_audio_with_tts(self, user_audio_path: str, tts_audio_path: str, 
                                expected_text: str) -> Dict:
        """
        ä½¿ç”¨ASRå°†ç”¨æˆ·å‘éŸ³ä¸TTSæ ‡å‡†å‘éŸ³è¿›è¡Œæ—¶é—´è½´å¯¹é½
        :param user_audio_path: ç”¨æˆ·éŸ³é¢‘è·¯å¾„
        :param tts_audio_path: TTSéŸ³é¢‘è·¯å¾„
        :param expected_text: æœŸæœ›æ–‡æœ¬
        :return: å¯¹é½ç»“æœ
        """
        try:
            print(f"ğŸ”„ å¼€å§‹ASRæ—¶é—´è½´å¯¹é½...")
            
            # 1. éªŒè¯ç”¨æˆ·å½•éŸ³è´¨é‡
            user_quality = self.validate_user_audio_quality(user_audio_path)
            if not user_quality['is_valid']:
                return {
                    'success': False,
                    'error': f"ç”¨æˆ·å½•éŸ³è´¨é‡é—®é¢˜: {user_quality['reason']}",
                    'details': user_quality['details']
                }
            
            # 2. è·å–TTSéŸ³é¢‘çš„æœ‰æ•ˆæ—¶é•¿
            tts_duration = self.get_tts_audio_duration(tts_audio_path, expected_text)
            
            # 3. ä½¿ç”¨ASRåˆ†æç”¨æˆ·éŸ³é¢‘
            user_asr_result = None
            if self.vad_processor:
                user_asr_result = self.vad_processor.recognize_speech_with_timestamps(
                    user_audio_path, expected_text
                )
            
            # 4. è·å–ç”¨æˆ·å½•éŸ³çš„è¯­éŸ³æ®µ
            user_speech_segments = []
            if self.vad_processor:
                vad_info = self.vad_processor.get_speech_regions_timestamps(user_audio_path)
                user_speech_segments = vad_info.get('speech_segments', [])
            
            # 5. è®¡ç®—å¯¹é½ç­–ç•¥
            alignment_result = self._calculate_temporal_alignment(
                user_asr_result, user_speech_segments, tts_duration, expected_text
            )
            
            return {
                'success': True,
                'tts_effective_duration': tts_duration,
                'user_quality': user_quality,
                'user_asr_result': user_asr_result,
                'user_speech_segments': user_speech_segments,
                'alignment': alignment_result
            }
            
        except Exception as e:
            print(f"âŒ ASRæ—¶é—´è½´å¯¹é½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'success': False,
                'error': f'ASRå¯¹é½è¿‡ç¨‹å¤±è´¥: {str(e)}'
            }
    
    def _calculate_temporal_alignment(self, user_asr_result: Optional[Dict], 
                                    user_speech_segments: List[Tuple], 
                                    tts_duration: float, 
                                    expected_text: str) -> Dict:
        """
        è®¡ç®—æ—¶é—´è½´å¯¹é½ç­–ç•¥
        :param user_asr_result: ç”¨æˆ·ASRè¯†åˆ«ç»“æœ
        :param user_speech_segments: ç”¨æˆ·è¯­éŸ³æ®µ
        :param tts_duration: TTSæœ‰æ•ˆæ—¶é•¿
        :param expected_text: æœŸæœ›æ–‡æœ¬
        :return: å¯¹é½ç­–ç•¥
        """
        try:
            # æ¸…ç†æœŸæœ›æ–‡æœ¬
            clean_text = ''.join(c for c in expected_text if c.strip() and c not in 'ï¼Œã€‚ï¼ï¼Ÿã€')
            char_count = len(clean_text)
            
            alignment_strategy = {
                'method': 'uniform',  # é»˜è®¤å‡åŒ€åˆ†å¸ƒ
                'user_start_time': 0.0,
                'user_end_time': tts_duration,
                'scaling_factor': 1.0,
                'time_offset': 0.0
            }
            
            # ç­–ç•¥1: å¦‚æœæœ‰ASRæ—¶é—´æˆ³ï¼Œä½¿ç”¨åŸºäºASRçš„å¯¹é½
            if (user_asr_result and user_asr_result.get('timestamps') and 
                len(user_asr_result['timestamps']) > 0):
                
                timestamps = user_asr_result['timestamps']
                first_word_start = timestamps[0]['start']
                last_word_end = timestamps[-1]['end']
                user_speech_duration = last_word_end - first_word_start
                
                if user_speech_duration > 0:
                    # è®¡ç®—ç¼©æ”¾å› å­ï¼ˆç”¨æˆ·è¯´è¯é€Ÿåº¦ vs TTSé€Ÿåº¦ï¼‰
                    scaling_factor = user_speech_duration / tts_duration
                    
                    alignment_strategy.update({
                        'method': 'asr_based',
                        'user_start_time': first_word_start,
                        'user_end_time': last_word_end,
                        'scaling_factor': scaling_factor,
                        'time_offset': first_word_start,
                        'asr_word_count': len(timestamps)
                    })
                    
                    print(f"âœ“ ä½¿ç”¨ASRå¯¹é½: ç”¨æˆ·è¯­éŸ³{user_speech_duration:.2f}s, TTS{tts_duration:.2f}s, ç¼©æ”¾{scaling_factor:.2f}")
            
            # ç­–ç•¥2: å¦‚æœæœ‰VADè¯­éŸ³æ®µï¼Œä½¿ç”¨åŸºäºVADçš„å¯¹é½
            elif user_speech_segments:
                # åˆå¹¶æ‰€æœ‰è¯­éŸ³æ®µ
                speech_start = min(start for start, end in user_speech_segments)
                speech_end = max(end for start, end in user_speech_segments)
                user_speech_duration = speech_end - speech_start
                
                if user_speech_duration > 0:
                    scaling_factor = user_speech_duration / tts_duration
                    
                    alignment_strategy.update({
                        'method': 'vad_based',
                        'user_start_time': speech_start,
                        'user_end_time': speech_end,
                        'scaling_factor': scaling_factor,
                        'time_offset': speech_start,
                        'speech_segments_count': len(user_speech_segments)
                    })
                    
                    print(f"âœ“ ä½¿ç”¨VADå¯¹é½: ç”¨æˆ·è¯­éŸ³{user_speech_duration:.2f}s, TTS{tts_duration:.2f}s, ç¼©æ”¾{scaling_factor:.2f}")
            
            # ç­–ç•¥3: ç®€å•çš„æ—¶é•¿å¯¹é½ï¼ˆç”¨æˆ·éŸ³é¢‘æ€»æ—¶é•¿ vs TTSæ—¶é•¿ï¼‰
            else:
                print("âš ï¸ ä½¿ç”¨ç®€å•æ—¶é•¿å¯¹é½ï¼ˆæ— ASRå’ŒVADä¿¡æ¯ï¼‰")
                alignment_strategy.update({
                    'method': 'duration_based',
                    'user_end_time': tts_duration  # å‡è®¾ç”¨æˆ·è¯´è¯æ—¶é•¿ç­‰äºTTSæ—¶é•¿
                })
            
            return alignment_strategy
            
        except Exception as e:
            print(f"âŒ è®¡ç®—æ—¶é—´è½´å¯¹é½å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤çš„å‡åŒ€å¯¹é½
            return {
                'method': 'uniform',
                'user_start_time': 0.0,
                'user_end_time': tts_duration,
                'scaling_factor': 1.0,
                'time_offset': 0.0
            }
    
    def truncate_pitch_curves_to_tts_duration(self, standard_pitch: Dict, user_pitch: Dict, 
                                            tts_duration: float, alignment: Dict) -> Dict:
        """
        å°†éŸ³é«˜æ›²çº¿æˆªæ–­åˆ°TTSæœ‰æ•ˆæ—¶é•¿ï¼Œå¹¶è¿›è¡Œæ—¶é—´å¯¹é½
        :param standard_pitch: æ ‡å‡†éŸ³é«˜æ•°æ®
        :param user_pitch: ç”¨æˆ·éŸ³é«˜æ•°æ®
        :param tts_duration: TTSæœ‰æ•ˆæ—¶é•¿
        :param alignment: å¯¹é½ç­–ç•¥
        :return: æˆªæ–­å’Œå¯¹é½åçš„éŸ³é«˜æ•°æ®
        """
        try:
            print(f"âœ‚ï¸ æˆªæ–­éŸ³é«˜æ›²çº¿åˆ°TTSæ—¶é•¿: {tts_duration:.3f}s")
            
            # 1. æˆªæ–­æ ‡å‡†éŸ³é«˜åˆ°TTSæœ‰æ•ˆæ—¶é•¿
            std_times = standard_pitch['times']
            std_pitch_values = standard_pitch['pitch_values']
            
            # æ‰¾åˆ°TTSæœ‰æ•ˆæ—¶é•¿å†…çš„æ ‡å‡†éŸ³é«˜ç‚¹
            tts_mask = std_times <= tts_duration
            truncated_std_times = std_times[tts_mask]
            truncated_std_pitch = std_pitch_values[tts_mask]
            
            print(f"âœ“ æ ‡å‡†éŸ³é«˜æˆªæ–­: {len(std_times)} -> {len(truncated_std_times)} ç‚¹")
            
            # 2. å¯¹ç”¨æˆ·éŸ³é«˜è¿›è¡Œæ—¶é—´è½´å¯¹é½å’Œæˆªæ–­
            user_times = user_pitch['times']
            user_pitch_values = user_pitch['pitch_values']
            
            # æ ¹æ®å¯¹é½ç­–ç•¥è°ƒæ•´ç”¨æˆ·æ—¶é—´è½´
            aligned_user_times, aligned_user_pitch = self._align_user_timeline(
                user_times, user_pitch_values, alignment, tts_duration
            )
            
            # 3. å°†ä¸¤ä¸ªéŸ³é«˜æ›²çº¿æ’å€¼åˆ°ç»Ÿä¸€çš„æ—¶é—´è½´
            if len(truncated_std_times) > 0 and len(aligned_user_times) > 0:
                # åˆ›å»ºç»Ÿä¸€æ—¶é—´è½´ï¼ˆä»0åˆ°TTSç»“æŸæ—¶é—´ï¼‰
                unified_times = np.linspace(0, tts_duration, 
                                          max(len(truncated_std_times), len(aligned_user_times), 200))
                
                # æ’å€¼æ ‡å‡†éŸ³é«˜åˆ°ç»Ÿä¸€æ—¶é—´è½´
                unified_std_pitch = self._interpolate_pitch_to_timeline(
                    truncated_std_times, truncated_std_pitch, unified_times
                )
                
                # æ’å€¼ç”¨æˆ·éŸ³é«˜åˆ°ç»Ÿä¸€æ—¶é—´è½´
                unified_user_pitch = self._interpolate_pitch_to_timeline(
                    aligned_user_times, aligned_user_pitch, unified_times
                )
                
                print(f"âœ“ éŸ³é«˜æ›²çº¿å¯¹é½å®Œæˆ: ç»Ÿä¸€æ—¶é—´è½´{len(unified_times)}ç‚¹")
                
                return {
                    'aligned_times': unified_times,
                    'aligned_standard': unified_std_pitch,
                    'aligned_user': unified_user_pitch,
                    'tts_duration': tts_duration,
                    'alignment_method': alignment['method'],
                    'success': True
                }
            else:
                print("âŒ éŸ³é«˜æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå¯¹é½")
                return {
                    'aligned_times': np.array([]),
                    'aligned_standard': np.array([]),
                    'aligned_user': np.array([]),
                    'tts_duration': tts_duration,
                    'alignment_method': 'failed',
                    'success': False
                }
                
        except Exception as e:
            print(f"âŒ éŸ³é«˜æ›²çº¿æˆªæ–­å¯¹é½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'aligned_times': np.array([]),
                'aligned_standard': np.array([]),
                'aligned_user': np.array([]),
                'tts_duration': tts_duration,
                'alignment_method': 'error',
                'success': False,
                'error': str(e)
            }
    
    def _align_user_timeline(self, user_times: np.ndarray, user_pitch: np.ndarray, 
                           alignment: Dict, tts_duration: float) -> Tuple[np.ndarray, np.ndarray]:
        """
        æ ¹æ®å¯¹é½ç­–ç•¥è°ƒæ•´ç”¨æˆ·éŸ³é«˜çš„æ—¶é—´è½´
        :param user_times: ç”¨æˆ·åŸå§‹æ—¶é—´è½´
        :param user_pitch: ç”¨æˆ·éŸ³é«˜å€¼
        :param alignment: å¯¹é½ç­–ç•¥
        :param tts_duration: TTSæ—¶é•¿
        :return: å¯¹é½åçš„æ—¶é—´è½´å’ŒéŸ³é«˜
        """
        try:
            method = alignment.get('method', 'uniform')
            
            if method == 'asr_based':
                # åŸºäºASRçš„ç²¾ç¡®å¯¹é½
                user_start = alignment['user_start_time']
                user_end = alignment['user_end_time']
                scaling_factor = alignment['scaling_factor']
                
                # æå–ç”¨æˆ·è¯­éŸ³æ—¶é—´æ®µå†…çš„éŸ³é«˜
                speech_mask = (user_times >= user_start) & (user_times <= user_end)
                speech_times = user_times[speech_mask]
                speech_pitch = user_pitch[speech_mask]
                
                if len(speech_times) > 0:
                    # å°†ç”¨æˆ·è¯­éŸ³æ—¶é—´æ˜ å°„åˆ°TTSæ—¶é—´è½´ï¼ˆ0åˆ°tts_durationï¼‰
                    normalized_times = (speech_times - user_start) / (user_end - user_start) * tts_duration
                    return normalized_times, speech_pitch
            
            elif method == 'vad_based':
                # åŸºäºVADçš„å¯¹é½
                user_start = alignment['user_start_time']
                user_end = alignment['user_end_time']
                
                # æå–è¯­éŸ³æ—¶é—´æ®µ
                speech_mask = (user_times >= user_start) & (user_times <= user_end)
                speech_times = user_times[speech_mask]
                speech_pitch = user_pitch[speech_mask]
                
                if len(speech_times) > 0:
                    # çº¿æ€§æ˜ å°„åˆ°TTSæ—¶é—´è½´
                    normalized_times = (speech_times - user_start) / (user_end - user_start) * tts_duration
                    return normalized_times, speech_pitch
            
            elif method == 'duration_based':
                # åŸºäºæ€»æ—¶é•¿çš„ç®€å•å¯¹é½
                max_user_time = user_times[-1] if len(user_times) > 0 else tts_duration
                # çº¿æ€§ç¼©æ”¾åˆ°TTSæ—¶é•¿
                scaled_times = user_times * (tts_duration / max_user_time)
                return scaled_times, user_pitch
            
            # é»˜è®¤ï¼šå‡åŒ€åˆ†å¸ƒå¯¹é½
            if len(user_times) > 0:
                # ç®€å•çº¿æ€§ç¼©æ”¾
                max_user_time = user_times[-1]
                if max_user_time > 0:
                    scaled_times = user_times * (tts_duration / max_user_time)
                    return scaled_times, user_pitch
            
            return user_times, user_pitch
            
        except Exception as e:
            print(f"âŒ ç”¨æˆ·æ—¶é—´è½´å¯¹é½å¤±è´¥: {e}")
            return user_times, user_pitch
    
    def _interpolate_pitch_to_timeline(self, source_times: np.ndarray, source_pitch: np.ndarray, 
                                     target_times: np.ndarray) -> np.ndarray:
        """
        å°†éŸ³é«˜æ•°æ®æ’å€¼åˆ°ç›®æ ‡æ—¶é—´è½´
        :param source_times: æºæ—¶é—´è½´
        :param source_pitch: æºéŸ³é«˜å€¼
        :param target_times: ç›®æ ‡æ—¶é—´è½´
        :return: æ’å€¼åçš„éŸ³é«˜å€¼
        """
        try:
            if len(source_times) == 0 or len(source_pitch) == 0:
                return np.full(len(target_times), np.nan)
            
            # è¿‡æ»¤æœ‰æ•ˆçš„éŸ³é«˜å€¼
            valid_mask = ~np.isnan(source_pitch) & (source_pitch > 0)
            if np.sum(valid_mask) < 2:
                return np.full(len(target_times), np.nan)
            
            valid_times = source_times[valid_mask]
            valid_pitch = source_pitch[valid_mask]
            
            # åˆ›å»ºæ’å€¼å‡½æ•°
            f = interp1d(valid_times, valid_pitch, kind='linear',
                        bounds_error=False, fill_value=np.nan)
            
            # æ’å€¼åˆ°ç›®æ ‡æ—¶é—´è½´
            interpolated_pitch = f(target_times)
            
            return interpolated_pitch
            
        except Exception as e:
            print(f"âŒ éŸ³é«˜æ’å€¼å¤±è´¥: {e}")
            return np.full(len(target_times), np.nan)


def test_enhanced_alignment():
    """æµ‹è¯•å¢å¼ºçš„éŸ³é«˜å¯¹é½åŠŸèƒ½"""
    print("=== æµ‹è¯•å¢å¼ºçš„éŸ³é«˜å¯¹é½åŠŸèƒ½ ===")
    
    aligner = EnhancedPitchAligner()
    
    # å¦‚æœæœ‰æµ‹è¯•éŸ³é¢‘æ–‡ä»¶
    test_tts_audio = "temp/test_tts.wav"
    test_user_audio = "temp/test_user.wav"
    test_text = "ä½ å¥½ä¸–ç•Œ"
    
    if os.path.exists(test_tts_audio) and os.path.exists(test_user_audio):
        print(f"æµ‹è¯•æ–‡ä»¶: TTS={test_tts_audio}, User={test_user_audio}")
        
        # æµ‹è¯•TTSæ—¶é•¿æ£€æµ‹
        tts_duration = aligner.get_tts_audio_duration(test_tts_audio, test_text)
        print(f"TTSæœ‰æ•ˆæ—¶é•¿: {tts_duration:.3f}s")
        
        # æµ‹è¯•ç”¨æˆ·å½•éŸ³è´¨é‡éªŒè¯
        quality_result = aligner.validate_user_audio_quality(test_user_audio)
        print(f"ç”¨æˆ·å½•éŸ³è´¨é‡: {quality_result}")
        
        # æµ‹è¯•ASRå¯¹é½
        alignment_result = aligner.align_user_audio_with_tts(
            test_user_audio, test_tts_audio, test_text
        )
        print(f"ASRå¯¹é½ç»“æœ: {alignment_result.get('success', False)}")
        
    else:
        print("æœªæ‰¾åˆ°æµ‹è¯•éŸ³é¢‘æ–‡ä»¶ï¼Œè·³è¿‡æµ‹è¯•")


if __name__ == "__main__":
    test_enhanced_alignment()
