#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºæ—¶é—´æˆ³æ¨¡å—
æä¾›å¤šç§æ—¶é—´æˆ³è·å–æ–¹æ³•ï¼šæœ¬åœ°ASR + äº‘ç«¯Fun-ASR + é™çº§æ–¹æ¡ˆ
"""

import os
import json
import time
import librosa
import jieba
from typing import Dict, List, Optional, Tuple

try:
    from dashscope.audio.asr import Transcription
    import dashscope
    from http import HTTPStatus
    DASHSCOPE_AVAILABLE = True
except ImportError:
    DASHSCOPE_AVAILABLE = False

class EnhancedTimestampProcessor:
    """å¢å¼ºçš„æ—¶é—´æˆ³å¤„ç†å™¨"""
    
    def __init__(self, api_key: str = None):
        self.api_key = api_key or os.environ.get('DASHSCOPE_API_KEY')
        
        # åˆå§‹åŒ–äº‘ç«¯æœåŠ¡
        if self.api_key and DASHSCOPE_AVAILABLE:
            dashscope.api_key = self.api_key
            self.cloud_available = True
            print("âœ“ äº‘ç«¯æ—¶é—´æˆ³æœåŠ¡å·²å¯ç”¨")
        else:
            self.cloud_available = False
            print("âš ï¸ äº‘ç«¯æ—¶é—´æˆ³æœåŠ¡ä¸å¯ç”¨")
        
        # å°è¯•è·å–æœ¬åœ°ASRæ¨¡å‹
        self.local_asr_model = None
        try:
            from vad_module import VADProcessor
            vad_processor = VADProcessor()
            if hasattr(vad_processor, 'local_asr_model') and vad_processor.local_asr_model:
                self.local_asr_model = vad_processor.local_asr_model
                print("âœ“ æœ¬åœ°ASRæ¨¡å‹å¯ç”¨")
            else:
                print("âš ï¸ æœ¬åœ°ASRæ¨¡å‹ä¸å¯ç”¨")
        except Exception as e:
            print(f"âš ï¸ æ— æ³•è®¿é—®æœ¬åœ°ASRæ¨¡å‹: {e}")
    
    def get_precise_word_timestamps(self, audio_path: str, expected_text: str = None) -> Dict:
        """
        è·å–ç²¾ç¡®çš„è¯çº§æ—¶é—´æˆ³
        æŒ‰ä¼˜å…ˆçº§å°è¯•ä¸åŒæ–¹æ³•
        """
        print(f"ğŸ¯ å¼€å§‹è·å–ç²¾ç¡®è¯çº§æ—¶é—´æˆ³: {os.path.basename(audio_path)}")
        
        # æ–¹æ³•1: æœ¬åœ°ASRæ¨¡å‹
        if self.local_asr_model:
            print("ğŸ“ å°è¯•æœ¬åœ°ASRæ¨¡å‹...")
            result = self._get_timestamps_local_asr(audio_path, expected_text)
            if result and result.get('success'):
                print("âœ… æœ¬åœ°ASRæ—¶é—´æˆ³è·å–æˆåŠŸ")
                return result
        
        # æ–¹æ³•2: äº‘ç«¯Fun-ASRï¼ˆéœ€è¦å…¬ç½‘URLï¼Œæš‚æ—¶è·³è¿‡ï¼‰
        # if self.cloud_available:
        #     print("â˜ï¸ å°è¯•äº‘ç«¯Fun-ASR...")
        #     # è¿™é‡Œå¯ä»¥æ·»åŠ æ–‡ä»¶ä¸Šä¼ é€»è¾‘
        
        # æ–¹æ³•3: æ™ºèƒ½é™çº§æ–¹æ¡ˆ
        print("ğŸ“Š ä½¿ç”¨æ™ºèƒ½é™çº§æ–¹æ¡ˆ...")
        result = self._get_timestamps_smart_fallback(audio_path, expected_text)
        if result and result.get('success'):
            print("âœ… æ™ºèƒ½é™çº§æ—¶é—´æˆ³è·å–æˆåŠŸ")
            return result
        
        # æ–¹æ³•4: åŸºç¡€é™çº§æ–¹æ¡ˆ
        print("ğŸ”§ ä½¿ç”¨åŸºç¡€é™çº§æ–¹æ¡ˆ...")
        return self._get_timestamps_basic_fallback(audio_path, expected_text)
    
    def _get_timestamps_local_asr(self, audio_path: str, expected_text: str = None) -> Dict:
        """ä½¿ç”¨æœ¬åœ°ASRæ¨¡å‹è·å–æ—¶é—´æˆ³"""
        try:
            # æ‰§è¡ŒASRè¯†åˆ«
            result = self.local_asr_model.generate(input=audio_path)
            
            if not result or not isinstance(result, list) or len(result) == 0:
                return {'success': False}
            
            # æå–è¯†åˆ«æ–‡æœ¬
            item = result[0]
            recognized_text = item.get('text', '')
            
            if not recognized_text:
                return {'success': False}
            
            # åˆ›å»ºè¯çº§æ—¶é—´æˆ³
            word_timestamps = self._create_smart_word_timestamps(
                recognized_text, audio_path, expected_text
            )
            
            return {
                'method': 'local_asr',
                'recognized_text': recognized_text,
                'expected_text': expected_text or '',
                'word_timestamps': word_timestamps,
                'success': True
            }
            
        except Exception as e:
            print(f"æœ¬åœ°ASRå¤„ç†å¤±è´¥: {e}")
            return {'success': False}
    
    def _get_timestamps_smart_fallback(self, audio_path: str, expected_text: str = None) -> Dict:
        """æ™ºèƒ½é™çº§æ–¹æ¡ˆï¼šç»“åˆVADå’ŒéŸ³é¢‘ç‰¹å¾"""
        try:
            if not expected_text:
                return {'success': False}
            
            # 1. è·å–éŸ³é¢‘åŸºæœ¬ä¿¡æ¯
            y, sr = librosa.load(audio_path)
            duration = len(y) / sr
            
            # 2. å°è¯•ä½¿ç”¨VADæ£€æµ‹è¯­éŸ³æ®µ
            speech_segments = self._detect_speech_with_energy(y, sr)
            
            # 3. åˆ†è¯
            words = list(jieba.cut(expected_text))
            words = [w.strip() for w in words if w.strip()]
            
            if not words:
                return {'success': False}
            
            # 4. æ™ºèƒ½åˆ†é…æ—¶é—´æˆ³
            word_timestamps = self._distribute_words_to_speech_segments(
                words, speech_segments, duration
            )
            
            return {
                'method': 'smart_fallback',
                'recognized_text': expected_text,
                'expected_text': expected_text,
                'word_timestamps': word_timestamps,
                'speech_segments': speech_segments,
                'success': True
            }
            
        except Exception as e:
            print(f"æ™ºèƒ½é™çº§æ–¹æ¡ˆå¤±è´¥: {e}")
            return {'success': False}
    
    def _get_timestamps_basic_fallback(self, audio_path: str, expected_text: str = None) -> Dict:
        """åŸºç¡€é™çº§æ–¹æ¡ˆï¼šå‡åŒ€æ—¶é—´åˆ†å¸ƒ"""
        try:
            # è·å–éŸ³é¢‘æ—¶é•¿
            if os.path.exists(audio_path):
                y, sr = librosa.load(audio_path)
                duration = len(y) / sr
            else:
                duration = 2.0
            
            text_to_use = expected_text or "é»˜è®¤æ–‡æœ¬"
            
            # åˆ†è¯
            words = list(jieba.cut(text_to_use))
            words = [w.strip() for w in words if w.strip()]
            
            if not words:
                words = [text_to_use]
            
            # å‡åŒ€åˆ†å¸ƒ
            word_timestamps = []
            time_per_word = duration / len(words)
            
            for i, word in enumerate(words):
                start_time = i * time_per_word
                end_time = (i + 1) * time_per_word
                word_timestamps.append({
                    'word': word,
                    'start_time': start_time,
                    'end_time': end_time,
                    'confidence': 0.5  # ä¸­ç­‰ç½®ä¿¡åº¦
                })
            
            return {
                'method': 'basic_fallback',
                'recognized_text': text_to_use,
                'expected_text': expected_text or '',
                'word_timestamps': word_timestamps,
                'success': True
            }
            
        except Exception as e:
            print(f"åŸºç¡€é™çº§æ–¹æ¡ˆå¤±è´¥: {e}")
            return {
                'method': 'error',
                'recognized_text': '',
                'expected_text': expected_text or '',
                'word_timestamps': [],
                'success': False
            }
    
    def _create_smart_word_timestamps(self, recognized_text: str, audio_path: str, expected_text: str = None) -> List[Dict]:
        """åˆ›å»ºæ™ºèƒ½è¯çº§æ—¶é—´æˆ³"""
        try:
            # è·å–éŸ³é¢‘æ—¶é•¿
            y, sr = librosa.load(audio_path)
            duration = len(y) / sr
            
            # åˆ†è¯
            words = list(jieba.cut(recognized_text))
            words = [w.strip() for w in words if w.strip()]
            
            if not words:
                return []
            
            # å¦‚æœæœ‰æœŸæœ›æ–‡æœ¬ï¼Œå°è¯•å¯¹é½
            if expected_text:
                expected_words = list(jieba.cut(expected_text))
                expected_words = [w.strip() for w in expected_words if w.strip()]
                
                # ç®€å•çš„è¯å¯¹é½
                if len(words) == len(expected_words):
                    # è¯æ•°ç›¸åŒï¼Œç›´æ¥å¯¹åº”
                    words = expected_words
                elif len(expected_words) > 0:
                    # æ··åˆä½¿ç”¨
                    words = expected_words
            
            # åˆ›å»ºæ—¶é—´æˆ³
            word_timestamps = []
            time_per_word = duration / len(words)
            
            for i, word in enumerate(words):
                start_time = i * time_per_word
                end_time = (i + 1) * time_per_word
                word_timestamps.append({
                    'word': word,
                    'start_time': start_time,
                    'end_time': end_time,
                    'confidence': 0.8  # è¾ƒé«˜ç½®ä¿¡åº¦
                })
            
            return word_timestamps
            
        except Exception as e:
            print(f"åˆ›å»ºæ™ºèƒ½æ—¶é—´æˆ³å¤±è´¥: {e}")
            return []
    
    def _detect_speech_with_energy(self, y, sr, frame_length=2048, hop_length=512):
        """ä½¿ç”¨èƒ½é‡æ£€æµ‹è¯­éŸ³æ®µ"""
        try:
            # è®¡ç®—çŸ­æ—¶èƒ½é‡
            frame_energy = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]
            
            # è®¡ç®—é˜ˆå€¼
            energy_threshold = 0.02 * frame_energy.max()
            
            # æ£€æµ‹è¯­éŸ³æ®µ
            speech_frames = frame_energy > energy_threshold
            
            # è½¬æ¢ä¸ºæ—¶é—´æ®µ
            times = librosa.frames_to_time(range(len(speech_frames)), sr=sr, hop_length=hop_length)
            
            speech_segments = []
            in_speech = False
            start_time = 0
            
            for i, is_speech in enumerate(speech_frames):
                if is_speech and not in_speech:
                    # è¯­éŸ³å¼€å§‹
                    start_time = times[i]
                    in_speech = True
                elif not is_speech and in_speech:
                    # è¯­éŸ³ç»“æŸ
                    end_time = times[i]
                    if end_time - start_time > 0.1:  # æœ€å°è¯­éŸ³æ®µé•¿åº¦
                        speech_segments.append((start_time, end_time))
                    in_speech = False
            
            # å¤„ç†æœ€åä¸€ä¸ªè¯­éŸ³æ®µ
            if in_speech:
                speech_segments.append((start_time, times[-1]))
            
            return speech_segments if speech_segments else [(0, len(y)/sr)]
            
        except Exception as e:
            print(f"è¯­éŸ³æ£€æµ‹å¤±è´¥: {e}")
            return [(0, len(y)/sr)]
    
    def _distribute_words_to_speech_segments(self, words: List[str], speech_segments: List[Tuple], total_duration: float) -> List[Dict]:
        """å°†è¯åˆ†é…åˆ°è¯­éŸ³æ®µä¸­"""
        try:
            if not speech_segments or not words:
                return []
            
            word_timestamps = []
            
            # è®¡ç®—æ€»è¯­éŸ³æ—¶é•¿
            total_speech_duration = sum(end - start for start, end in speech_segments)
            
            # ä¸ºæ¯ä¸ªè¯åˆ†é…æ—¶é—´
            chars_per_word = [len(word) for word in words]
            total_chars = sum(chars_per_word)
            
            current_time = speech_segments[0][0]
            segment_idx = 0
            segment_time_left = speech_segments[0][1] - speech_segments[0][0]
            
            for i, word in enumerate(words):
                if total_chars > 0:
                    word_duration = (chars_per_word[i] / total_chars) * total_speech_duration
                else:
                    word_duration = total_speech_duration / len(words)
                
                # ç¡®ä¿è¯åœ¨å½“å‰è¯­éŸ³æ®µå†…
                while segment_time_left < word_duration and segment_idx < len(speech_segments) - 1:
                    # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªè¯­éŸ³æ®µ
                    segment_idx += 1
                    current_time = speech_segments[segment_idx][0]
                    segment_time_left = speech_segments[segment_idx][1] - speech_segments[segment_idx][0]
                
                start_time = current_time
                end_time = min(current_time + word_duration, speech_segments[segment_idx][1])
                
                word_timestamps.append({
                    'word': word,
                    'start_time': start_time,
                    'end_time': end_time,
                    'confidence': 0.7
                })
                
                current_time = end_time
                segment_time_left -= (end_time - start_time)
            
            return word_timestamps
            
        except Exception as e:
            print(f"è¯æ—¶é—´åˆ†é…å¤±è´¥: {e}")
            return []

def test_enhanced_timestamps():
    """æµ‹è¯•å¢å¼ºæ—¶é—´æˆ³åŠŸèƒ½"""
    processor = EnhancedTimestampProcessor()
    
    test_file = "outputs/fun_asr_test_standard_ä»Šå¤©å¤©æ°”å¾ˆå¥½.wav"
    expected_text = "ä»Šå¤©å¤©æ°”å¾ˆå¥½"
    
    if not os.path.exists(test_file):
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return
    
    print(f"ğŸµ æµ‹è¯•éŸ³é¢‘: {test_file}")
    print(f"ğŸ“ æœŸæœ›æ–‡æœ¬: {expected_text}")
    
    result = processor.get_precise_word_timestamps(test_file, expected_text)
    
    if result and result.get('success'):
        print(f"\nâœ… å¢å¼ºæ—¶é—´æˆ³è·å–æˆåŠŸ!")
        print(f"æ–¹æ³•: {result['method']}")
        print(f"è¯†åˆ«æ–‡æœ¬: {result['recognized_text']}")
        print(f"è¯çº§æ—¶é—´æˆ³æ•°é‡: {len(result['word_timestamps'])}")
        
        print("\nğŸ“Š è¯çº§æ—¶é—´æˆ³è¯¦æƒ…:")
        for i, ts in enumerate(result['word_timestamps'], 1):
            print(f"  {i}. '{ts['word']}': {ts['start_time']:.2f}s - {ts['end_time']:.2f}s (ç½®ä¿¡åº¦: {ts.get('confidence', 0.5):.1f})")
        
        return result
    else:
        print("âŒ å¢å¼ºæ—¶é—´æˆ³è·å–å¤±è´¥")
        return None

if __name__ == "__main__":
    test_enhanced_timestamps()
