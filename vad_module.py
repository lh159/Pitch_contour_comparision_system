# -*- coding: utf-8 -*-
"""
è¯­éŸ³æ´»åŠ¨æ£€æµ‹(VAD)æ¨¡å—
é›†æˆé˜¿é‡Œè¾¾æ‘©é™¢paraformer-v2è¿›è¡Œç²¾ç¡®çš„è¯­éŸ³æ´»åŠ¨åŒºåŸŸæ£€æµ‹
ç”¨äºæ”¹è¿›éŸ³é«˜æ›²çº¿æ¯”å¯¹çš„æ—¶åŸŸå¯¹é½ç²¾åº¦
"""
import os
import numpy as np
import librosa
import soundfile as sf
from typing import List, Tuple, Dict, Optional
import tempfile
import traceback

from config import Config

try:
    import dashscope
    from funasr import AutoModel
    DASHSCOPE_AVAILABLE = True
    FUNASR_AVAILABLE = True
except ImportError as e:
    print(f"è­¦å‘Š: é˜¿é‡Œè¯­éŸ³æœåŠ¡å¯¼å…¥å¤±è´¥: {e}")
    DASHSCOPE_AVAILABLE = False
    FUNASR_AVAILABLE = False

try:
    import jieba
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False
    print("è­¦å‘Š: jiebaåˆ†è¯å·¥å…·æœªå®‰è£…ï¼Œæ–‡æœ¬å¯¹é½åŠŸèƒ½å°†å—é™")

class VADProcessor:
    """è¯­éŸ³æ´»åŠ¨æ£€æµ‹å¤„ç†å™¨ (å•ä¾‹æ¨¡å¼)"""
    
    _instance = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        # é¿å…é‡å¤åˆå§‹åŒ–
        if self._initialized:
            return
            
        self.api_key = Config.ALIBABA_PARAFORMER_API_KEY
        self.vad_model_name = Config.ALIBABA_VAD_MODEL
        self.asr_model_name = Config.ALIBABA_ASR_MODEL
        
        # æœ¬åœ°VADæ¨¡å‹
        self.local_vad_model = None
        self.vad_available = False
        
        # æœ¬åœ°ASRæ¨¡å‹
        self.local_asr_model = None
        self.asr_available = False
        
        # åˆå§‹åŒ–æœåŠ¡
        self._initialize_services()
        self._initialized = True
    
    def _initialize_services(self):
        """åˆå§‹åŒ–è¯­éŸ³æœåŠ¡"""
        try:
            # åˆå§‹åŒ–DashScope (ç”¨äºparaformer-v2)
            if DASHSCOPE_AVAILABLE and self.api_key:
                dashscope.api_key = self.api_key
                print("âœ“ DashScope APIå·²é…ç½®")
            else:
                print("âš ï¸ DashScope APIæœªé…ç½®æˆ–ä¸å¯ç”¨")
            
            # åˆå§‹åŒ–æœ¬åœ°VADæ¨¡å‹ (FunASR)
            if FUNASR_AVAILABLE:
                try:
                    print("æ­£åœ¨åŠ è½½æœ¬åœ°VADæ¨¡å‹...")
                    self.local_vad_model = AutoModel(
                        model=self.vad_model_name,
                        model_revision="v2.0.4"
                    )
                    self.vad_available = True
                    print("âœ“ æœ¬åœ°VADæ¨¡å‹åŠ è½½æˆåŠŸ")
                except Exception as e:
                    print(f"âš ï¸ æœ¬åœ°VADæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                    print("å°†ä½¿ç”¨åŸºç¡€èƒ½é‡æ£€æµ‹æ–¹æ³•")
                
                # åˆå§‹åŒ–æœ¬åœ°ASRæ¨¡å‹
                try:
                    print(f"æ­£åœ¨åŠ è½½æœ¬åœ°ASRæ¨¡å‹: {self.asr_model_name}")
                    self.local_asr_model = AutoModel(
                        model=self.asr_model_name,
                        model_revision="v2.0.4"
                    )
                    self.asr_available = True
                    print("âœ“ æœ¬åœ°ASRæ¨¡å‹åŠ è½½æˆåŠŸ")
                except Exception as e:
                    print(f"âš ï¸ æœ¬åœ°ASRæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                    print("å°†ä½¿ç”¨VAD+uniformä¼°ç®—æ–¹æ³•ä½œä¸ºé™çº§æ–¹æ¡ˆ")
                    # ä¸è®¾ç½®ä¸ºå®Œå…¨ä¸å¯ç”¨ï¼Œè€Œæ˜¯ä½¿ç”¨é™çº§æ–¹æ¡ˆ
            
        except Exception as e:
            print(f"VADæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            traceback.print_exc()
    
    def detect_speech_segments(self, audio_path: str, method: str = 'auto') -> List[Tuple[float, float]]:
        """
        æ£€æµ‹éŸ³é¢‘ä¸­çš„è¯­éŸ³æ´»åŠ¨æ®µ
        :param audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        :param method: æ£€æµ‹æ–¹æ³• ('funasr', 'energy', 'auto')
        :return: è¯­éŸ³æ®µåˆ—è¡¨ [(å¼€å§‹æ—¶é—´, ç»“æŸæ—¶é—´), ...]
        """
        if method == 'auto':
            # è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ–¹æ³•
            if self.vad_available:
                method = 'funasr'
            else:
                method = 'energy'
        
        print(f"ä½¿ç”¨ {method} æ–¹æ³•è¿›è¡ŒVADæ£€æµ‹...")
        
        if method == 'funasr' and self.vad_available:
            return self._detect_with_funasr(audio_path)
        else:
            return self._detect_with_energy(audio_path)
    
    def _detect_with_funasr(self, audio_path: str) -> List[Tuple[float, float]]:
        """ä½¿ç”¨FunASR VADæ¨¡å‹æ£€æµ‹è¯­éŸ³æ®µ"""
        try:
            # ä½¿ç”¨æœ¬åœ°VADæ¨¡å‹
            result = self.local_vad_model.generate(input=audio_path)
            
            # è§£æç»“æœ
            speech_segments = []
            if isinstance(result, list) and len(result) > 0:
                # resultæ ¼å¼: [{'value': [[beg1, end1], [beg2, end2], ...]}]
                if 'value' in result[0]:
                    segments = result[0]['value']
                    for segment in segments:
                        if len(segment) >= 2:
                            # è½¬æ¢ä¸ºç§’ (FunASRè¿”å›æ¯«ç§’)
                            start_time = segment[0] / 1000.0
                            end_time = segment[1] / 1000.0
                            speech_segments.append((start_time, end_time))
            
            print(f"FunASR VADæ£€æµ‹åˆ° {len(speech_segments)} ä¸ªè¯­éŸ³æ®µ")
            return speech_segments
            
        except Exception as e:
            print(f"FunASR VADæ£€æµ‹å¤±è´¥: {e}")
            # é™çº§åˆ°èƒ½é‡æ£€æµ‹
            return self._detect_with_energy(audio_path)
    
    def _detect_with_energy(self, audio_path: str) -> List[Tuple[float, float]]:
        """ä½¿ç”¨èƒ½é‡é˜ˆå€¼æ–¹æ³•æ£€æµ‹è¯­éŸ³æ®µ"""
        try:
            # åŠ è½½éŸ³é¢‘
            y, sr = librosa.load(audio_path, sr=None)
            
            # è®¡ç®—çŸ­æ—¶èƒ½é‡
            frame_length = int(0.025 * sr)  # 25mså¸§
            hop_length = int(0.010 * sr)    # 10msè·³è·ƒ
            
            # è®¡ç®—RMSèƒ½é‡
            rms = librosa.feature.rms(
                y=y, 
                frame_length=frame_length, 
                hop_length=hop_length
            )[0]
            
            # åŠ¨æ€é˜ˆå€¼
            energy_mean = np.mean(rms)
            energy_std = np.std(rms)
            threshold = max(Config.VAD_ENERGY_THRESHOLD, 
                          energy_mean - 0.5 * energy_std)
            
            # æ£€æµ‹è¯­éŸ³å¸§
            speech_frames = rms > threshold
            
            # è½¬æ¢ä¸ºæ—¶é—´æ®µ
            times = librosa.frames_to_time(
                np.arange(len(speech_frames)), 
                sr=sr, 
                hop_length=hop_length
            )
            
            # åˆå¹¶è¿ç»­çš„è¯­éŸ³æ®µ
            speech_segments = self._merge_segments(
                times, speech_frames, 
                min_duration=Config.VAD_MIN_SPEECH_DURATION,
                max_gap=Config.VAD_MAX_SILENCE_DURATION
            )
            
            print(f"èƒ½é‡VADæ£€æµ‹åˆ° {len(speech_segments)} ä¸ªè¯­éŸ³æ®µ")
            return speech_segments
            
        except Exception as e:
            print(f"èƒ½é‡VADæ£€æµ‹å¤±è´¥: {e}")
            # è¿”å›æ•´ä¸ªéŸ³é¢‘ä½œä¸ºä¸€ä¸ªè¯­éŸ³æ®µ
            try:
                duration = librosa.get_duration(filename=audio_path)
                return [(0.0, duration)]
            except:
                return [(0.0, 10.0)]  # é»˜è®¤10ç§’
    
    def _merge_segments(self, times: np.ndarray, speech_frames: np.ndarray, 
                       min_duration: float = 0.1, max_gap: float = 0.5) -> List[Tuple[float, float]]:
        """åˆå¹¶è¿ç»­çš„è¯­éŸ³æ®µ"""
        segments = []
        in_speech = False
        start_time = None
        
        for i, (time, is_speech) in enumerate(zip(times, speech_frames)):
            if is_speech and not in_speech:
                # è¯­éŸ³å¼€å§‹
                start_time = time
                in_speech = True
            elif not is_speech and in_speech:
                # è¯­éŸ³ç»“æŸ
                if start_time is not None:
                    duration = time - start_time
                    if duration >= min_duration:
                        segments.append((start_time, time))
                in_speech = False
        
        # å¤„ç†æœ€åä¸€ä¸ªæ®µ
        if in_speech and start_time is not None:
            duration = times[-1] - start_time
            if duration >= min_duration:
                segments.append((start_time, times[-1]))
        
        # åˆå¹¶é—´éš”è¿‡å°çš„æ®µ
        merged_segments = []
        for start, end in segments:
            if not merged_segments:
                merged_segments.append((start, end))
            else:
                last_start, last_end = merged_segments[-1]
                gap = start - last_end
                if gap <= max_gap:
                    # åˆå¹¶æ®µ
                    merged_segments[-1] = (last_start, end)
                else:
                    merged_segments.append((start, end))
        
        return merged_segments
    
    def get_speech_regions_timestamps(self, audio_path: str) -> Dict:
        """
        è·å–è¯­éŸ³åŒºåŸŸçš„è¯¦ç»†æ—¶é—´æˆ³ä¿¡æ¯
        :param audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        :return: åŒ…å«å„ç§æ—¶é—´æˆ³ä¿¡æ¯çš„å­—å…¸
        """
        try:
            # æ£€æµ‹è¯­éŸ³æ®µ
            speech_segments = self.detect_speech_segments(audio_path)
            
            # åŠ è½½éŸ³é¢‘è·å–æ€»æ—¶é•¿
            duration = librosa.get_duration(filename=audio_path)
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            total_speech_duration = sum(end - start for start, end in speech_segments)
            speech_ratio = total_speech_duration / duration if duration > 0 else 0
            
            # å¦‚æœä½¿ç”¨paraformer-v2è¿˜å¯ä»¥è·å–æ›´è¯¦ç»†çš„æ—¶é—´æˆ³ä¿¡æ¯
            detailed_timestamps = None
            if DASHSCOPE_AVAILABLE and self.api_key:
                detailed_timestamps = self._get_detailed_timestamps(audio_path)
            
            return {
                'speech_segments': speech_segments,
                'total_duration': duration,
                'speech_duration': total_speech_duration,
                'silence_duration': duration - total_speech_duration,
                'speech_ratio': speech_ratio,
                'detailed_timestamps': detailed_timestamps,
                'segment_count': len(speech_segments)
            }
            
        except Exception as e:
            print(f"è·å–è¯­éŸ³æ—¶é—´æˆ³å¤±è´¥: {e}")
            return {
                'speech_segments': [(0.0, 0.0)],
                'total_duration': 0.0,
                'speech_duration': 0.0,
                'silence_duration': 0.0,
                'speech_ratio': 0.0,
                'detailed_timestamps': None,
                'segment_count': 0
            }
    
    def _get_detailed_timestamps(self, audio_path: str) -> Optional[Dict]:
        """ä½¿ç”¨paraformer-v2è·å–è¯¦ç»†çš„è¯çº§æ—¶é—´æˆ³(éœ€è¦ä¸Šä¼ åˆ°äº‘ç«¯)"""
        try:
            # æ³¨æ„: paraformer-v2éœ€è¦éŸ³é¢‘æ–‡ä»¶å¯é€šè¿‡å…¬ç½‘è®¿é—®
            # è¿™é‡Œä»…ä½œä¸ºç¤ºä¾‹ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦å…ˆä¸Šä¼ éŸ³é¢‘æ–‡ä»¶åˆ°OSSç­‰æœåŠ¡
            print("è·å–è¯¦ç»†æ—¶é—´æˆ³éœ€è¦å°†éŸ³é¢‘ä¸Šä¼ åˆ°äº‘ç«¯ï¼Œå½“å‰è·³è¿‡æ­¤åŠŸèƒ½")
            return None
            
        except Exception as e:
            print(f"è·å–è¯¦ç»†æ—¶é—´æˆ³å¤±è´¥: {e}")
            return None
    
    def recognize_speech_with_timestamps(self, audio_path: str) -> Optional[Dict]:
        """
        ä½¿ç”¨ASRæ¨¡å‹è¯†åˆ«è¯­éŸ³å¹¶è·å–æ—¶é—´æˆ³
        :param audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        :return: è¯†åˆ«ç»“æœå’Œæ—¶é—´æˆ³ä¿¡æ¯
        """
        if not self.asr_available:
            print("ASRæ¨¡å‹ä¸å¯ç”¨ï¼Œæ— æ³•è·å–è¯­éŸ³è¯†åˆ«ç»“æœ")
            return None
        
        try:
            print("æ­£åœ¨è¿›è¡Œè¯­éŸ³è¯†åˆ«...")
            result = self.local_asr_model.generate(
                input=audio_path,
                batch_size_s=300,  # æ‰¹å¤„ç†å¤§å°
                hotword=""  # çƒ­è¯
            )
            
            if isinstance(result, list) and len(result) > 0:
                # å¤„ç†è¯†åˆ«ç»“æœ
                recognition_result = result[0]
                
                # æå–æ–‡æœ¬
                recognized_text = recognition_result.get('text', '')
                
                # æå–æ—¶é—´æˆ³ (å¦‚æœå¯ç”¨)
                timestamps = []
                if 'timestamp' in recognition_result:
                    timestamp_data = recognition_result['timestamp']
                    if isinstance(timestamp_data, list):
                        for ts_item in timestamp_data:
                            if isinstance(ts_item, list) and len(ts_item) >= 3:
                                # [word, start_time, end_time]
                                word = ts_item[0]
                                start_time = ts_item[1] / 1000.0  # è½¬æ¢ä¸ºç§’
                                end_time = ts_item[2] / 1000.0
                                timestamps.append({
                                    'word': word,
                                    'start': start_time,
                                    'end': end_time
                                })
                
                print(f"è¯†åˆ«æ–‡æœ¬: {recognized_text}")
                print(f"æ—¶é—´æˆ³æ•°é‡: {len(timestamps)}")
                
                return {
                    'text': recognized_text,
                    'timestamps': timestamps,
                    'confidence': recognition_result.get('confidence', 0.0),
                    'raw_result': recognition_result
                }
            
            return None
            
        except Exception as e:
            print(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
            traceback.print_exc()
            return None
    
    def align_text_with_vad(self, expected_text: str, audio_path: str) -> Dict:
        """
        å°†æœŸæœ›æ–‡æœ¬ä¸è¯­éŸ³æ´»åŠ¨åŒºåŸŸå’Œè¯†åˆ«ç»“æœè¿›è¡Œå¯¹é½
        :param expected_text: æœŸæœ›çš„æ–‡æœ¬ï¼ˆå¦‚TTSçš„åŸæ–‡ï¼‰
        :param audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        :return: å¯¹é½ç»“æœ
        """
        try:
            print(f"æ­£åœ¨å¯¹é½æ–‡æœ¬: {expected_text}")
            
            # 1. è·å–VADç»“æœ
            vad_segments = self.detect_speech_segments(audio_path)
            
            # 2. è·å–ASRè¯†åˆ«ç»“æœ
            asr_result = self.recognize_speech_with_timestamps(audio_path)
            
            # 3. å‡†å¤‡æ–‡æœ¬å¯¹é½
            aligned_result = {
                'expected_text': expected_text,
                'vad_segments': vad_segments,
                'asr_result': asr_result,
                'text_alignment': []
            }
            
            if asr_result and asr_result['timestamps']:
                # 4. æ‰§è¡Œæ–‡æœ¬å¯¹é½
                alignment = self._align_expected_with_recognized(
                    expected_text, 
                    asr_result['text'],
                    asr_result['timestamps']
                )
                aligned_result['text_alignment'] = alignment
                
                # 5. å°†å¯¹é½ç»“æœæ˜ å°„åˆ°VADæ®µ
                aligned_result['vad_text_mapping'] = self._map_text_to_vad_segments(
                    alignment, vad_segments
                )
            
            return aligned_result
            
        except Exception as e:
            print(f"æ–‡æœ¬å¯¹é½å¤±è´¥: {e}")
            traceback.print_exc()
            return {
                'expected_text': expected_text,
                'vad_segments': vad_segments if 'vad_segments' in locals() else [],
                'asr_result': None,
                'text_alignment': [],
                'error': str(e)
            }
    
    def _align_expected_with_recognized(self, expected: str, recognized: str, timestamps: List[Dict]) -> List[Dict]:
        """
        å¯¹é½æœŸæœ›æ–‡æœ¬å’Œè¯†åˆ«æ–‡æœ¬
        :param expected: æœŸæœ›æ–‡æœ¬
        :param recognized: è¯†åˆ«æ–‡æœ¬  
        :param timestamps: è¯†åˆ«ç»“æœçš„æ—¶é—´æˆ³
        :return: å¯¹é½ç»“æœ
        """
        try:
            # æ¸…ç†å’Œåˆ†è¯
            if JIEBA_AVAILABLE:
                expected_words = list(jieba.cut(expected.strip()))
                recognized_words = list(jieba.cut(recognized.strip()))
            else:
                # ç®€å•æŒ‰å­—ç¬¦åˆ†å‰²
                expected_words = list(expected.strip())
                recognized_words = list(recognized.strip())
            
            # ç®€å•çš„å¯¹é½ç®—æ³•ï¼ˆå¯ä»¥æ”¹è¿›ä¸ºæ›´å¤æ‚çš„ç®—æ³•ï¼‰
            alignment = []
            expected_idx = 0
            timestamp_idx = 0
            
            while expected_idx < len(expected_words) and timestamp_idx < len(timestamps):
                expected_word = expected_words[expected_idx]
                timestamp_item = timestamps[timestamp_idx]
                recognized_word = timestamp_item['word']
                
                # å­—ç¬¦åŒ¹é…
                if expected_word == recognized_word or expected_word in recognized_word:
                    # å®Œå…¨åŒ¹é…
                    alignment.append({
                        'expected_word': expected_word,
                        'recognized_word': recognized_word,
                        'start_time': timestamp_item['start'],
                        'end_time': timestamp_item['end'],
                        'match_type': 'exact'
                    })
                    expected_idx += 1
                    timestamp_idx += 1
                
                elif recognized_word in expected_word:
                    # éƒ¨åˆ†åŒ¹é…
                    alignment.append({
                        'expected_word': expected_word,
                        'recognized_word': recognized_word,
                        'start_time': timestamp_item['start'],
                        'end_time': timestamp_item['end'],
                        'match_type': 'partial'
                    })
                    expected_idx += 1
                    timestamp_idx += 1
                
                else:
                    # ä¸åŒ¹é…ï¼Œå¯èƒ½æ˜¯æ’å…¥æˆ–åˆ é™¤
                    if len(recognized_word) > len(expected_word):
                        # è¯†åˆ«ç»“æœæ›´é•¿ï¼Œå¯èƒ½æ˜¯æ’å…¥
                        alignment.append({
                            'expected_word': '',
                            'recognized_word': recognized_word,
                            'start_time': timestamp_item['start'],
                            'end_time': timestamp_item['end'],
                            'match_type': 'insertion'
                        })
                        timestamp_idx += 1
                    else:
                        # æœŸæœ›æ–‡æœ¬æ›´é•¿ï¼Œå¯èƒ½æ˜¯åˆ é™¤
                        alignment.append({
                            'expected_word': expected_word,
                            'recognized_word': '',
                            'start_time': None,
                            'end_time': None,
                            'match_type': 'deletion'
                        })
                        expected_idx += 1
            
            # å¤„ç†å‰©ä½™çš„æœŸæœ›è¯æ±‡ï¼ˆåˆ é™¤ï¼‰
            while expected_idx < len(expected_words):
                alignment.append({
                    'expected_word': expected_words[expected_idx],
                    'recognized_word': '',
                    'start_time': None,
                    'end_time': None,
                    'match_type': 'deletion'
                })
                expected_idx += 1
            
            # å¤„ç†å‰©ä½™çš„è¯†åˆ«è¯æ±‡ï¼ˆæ’å…¥ï¼‰
            while timestamp_idx < len(timestamps):
                timestamp_item = timestamps[timestamp_idx]
                alignment.append({
                    'expected_word': '',
                    'recognized_word': timestamp_item['word'],
                    'start_time': timestamp_item['start'],
                    'end_time': timestamp_item['end'],
                    'match_type': 'insertion'
                })
                timestamp_idx += 1
            
            return alignment
            
        except Exception as e:
            print(f"æ–‡æœ¬å¯¹é½å¤„ç†å¤±è´¥: {e}")
            return []
    
    def _map_text_to_vad_segments(self, alignment: List[Dict], vad_segments: List[Tuple[float, float]]) -> List[Dict]:
        """
        å°†æ–‡æœ¬å¯¹é½ç»“æœæ˜ å°„åˆ°VADæ®µ
        :param alignment: æ–‡æœ¬å¯¹é½ç»“æœ
        :param vad_segments: VADè¯­éŸ³æ®µ
        :return: æ˜ å°„ç»“æœ
        """
        try:
            vad_mapping = []
            
            for vad_start, vad_end in vad_segments:
                # æ‰¾åˆ°è¯¥VADæ®µå†…çš„æ‰€æœ‰æ–‡æœ¬
                segment_words = []
                
                for align_item in alignment:
                    if (align_item['start_time'] is not None and 
                        align_item['end_time'] is not None):
                        word_start = align_item['start_time']
                        word_end = align_item['end_time']
                        
                        # æ£€æŸ¥è¯æ±‡æ˜¯å¦åœ¨å½“å‰VADæ®µå†…
                        if (word_start >= vad_start and word_start <= vad_end) or \
                           (word_end >= vad_start and word_end <= vad_end) or \
                           (word_start <= vad_start and word_end >= vad_end):
                            segment_words.append(align_item)
                
                # æ„å»ºè¯¥æ®µçš„æ–‡æœ¬
                expected_text = ''.join([w['expected_word'] for w in segment_words if w['expected_word']])
                recognized_text = ''.join([w['recognized_word'] for w in segment_words if w['recognized_word']])
                
                vad_mapping.append({
                    'vad_start': vad_start,
                    'vad_end': vad_end,
                    'expected_text': expected_text,
                    'recognized_text': recognized_text,
                    'words': segment_words,
                    'word_count': len(segment_words),
                    'match_quality': self._calculate_segment_match_quality(segment_words)
                })
            
            return vad_mapping
            
        except Exception as e:
            print(f"VADæ–‡æœ¬æ˜ å°„å¤±è´¥: {e}")
            return []
    
    def _calculate_segment_match_quality(self, words: List[Dict]) -> float:
        """è®¡ç®—æ®µè½åŒ¹é…è´¨é‡"""
        if not words:
            return 0.0
        
        exact_matches = sum(1 for w in words if w['match_type'] == 'exact')
        partial_matches = sum(1 for w in words if w['match_type'] == 'partial')
        total_words = len(words)
        
        # è®¡ç®—åŒ¹é…è´¨é‡åˆ†æ•°
        quality = (exact_matches + 0.5 * partial_matches) / total_words
        return quality
    
    def extract_speech_audio(self, audio_path: str, output_path: str = None) -> str:
        """
        æå–çº¯è¯­éŸ³éŸ³é¢‘ï¼ˆå»é™¤é™éŸ³æ®µï¼‰
        :param audio_path: è¾“å…¥éŸ³é¢‘è·¯å¾„
        :param output_path: è¾“å‡ºéŸ³é¢‘è·¯å¾„
        :return: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        try:
            # æ£€æµ‹è¯­éŸ³æ®µ
            speech_segments = self.detect_speech_segments(audio_path)
            
            if not speech_segments:
                print("æœªæ£€æµ‹åˆ°è¯­éŸ³æ®µï¼Œè¿”å›åŸéŸ³é¢‘")
                return audio_path
            
            # åŠ è½½éŸ³é¢‘
            y, sr = librosa.load(audio_path, sr=None)
            
            # æå–è¯­éŸ³æ®µ
            speech_audio = []
            for start_time, end_time in speech_segments:
                start_sample = int(start_time * sr)
                end_sample = int(end_time * sr)
                segment = y[start_sample:end_sample]
                speech_audio.append(segment)
            
            # åˆå¹¶æ‰€æœ‰è¯­éŸ³æ®µ
            if speech_audio:
                combined_audio = np.concatenate(speech_audio)
            else:
                combined_audio = y  # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ï¼Œä½¿ç”¨åŸéŸ³é¢‘
            
            # ä¿å­˜ç»“æœ
            if output_path is None:
                output_path = audio_path.replace('.wav', '_speech_only.wav')
            
            sf.write(output_path, combined_audio, sr)
            print(f"æå–çš„çº¯è¯­éŸ³éŸ³é¢‘å·²ä¿å­˜åˆ°: {output_path}")
            
            return output_path
            
        except Exception as e:
            print(f"æå–è¯­éŸ³éŸ³é¢‘å¤±è´¥: {e}")
            return audio_path  # è¿”å›åŸæ–‡ä»¶

class VADComparator:
    """åŸºäºVADçš„éŸ³é«˜æ¯”å¯¹å¢å¼ºå™¨"""
    
    def __init__(self):
        self.vad_processor = VADProcessor()
    
    def align_speech_regions(self, standard_audio: str, user_audio: str) -> Dict:
        """
        åŸºäºVADæ£€æµ‹ç»“æœå¯¹é½ä¸¤ä¸ªéŸ³é¢‘çš„è¯­éŸ³åŒºåŸŸ
        :param standard_audio: æ ‡å‡†éŸ³é¢‘è·¯å¾„
        :param user_audio: ç”¨æˆ·éŸ³é¢‘è·¯å¾„
        :return: å¯¹é½ç»“æœ
        """
        try:
            print("ğŸ¯ å¼€å§‹VADå¢å¼ºçš„éŸ³é¢‘å¯¹é½...")
            
            # æ£€æµ‹ä¸¤ä¸ªéŸ³é¢‘çš„è¯­éŸ³åŒºåŸŸ
            std_info = self.vad_processor.get_speech_regions_timestamps(standard_audio)
            user_info = self.vad_processor.get_speech_regions_timestamps(user_audio)
            
            print(f"æ ‡å‡†éŸ³é¢‘: {len(std_info['speech_segments'])} ä¸ªè¯­éŸ³æ®µ, "
                  f"è¯­éŸ³æ¯”ä¾‹: {std_info['speech_ratio']:.2%}")
            print(f"ç”¨æˆ·éŸ³é¢‘: {len(user_info['speech_segments'])} ä¸ªè¯­éŸ³æ®µ, "
                  f"è¯­éŸ³æ¯”ä¾‹: {user_info['speech_ratio']:.2%}")
            
            # æå–çº¯è¯­éŸ³éŸ³é¢‘ç”¨äºæ›´ç²¾ç¡®çš„éŸ³é«˜æ¯”å¯¹
            std_speech_path = self.vad_processor.extract_speech_audio(
                standard_audio, 
                standard_audio.replace('.wav', '_vad_speech.wav')
            )
            user_speech_path = self.vad_processor.extract_speech_audio(
                user_audio,
                user_audio.replace('.wav', '_vad_speech.wav')
            )
            
            return {
                'success': True,
                'standard_info': std_info,
                'user_info': user_info,
                'standard_speech_audio': std_speech_path,
                'user_speech_audio': user_speech_path,
                'alignment_quality': self._calculate_alignment_quality(std_info, user_info)
            }
            
        except Exception as e:
            print(f"VADå¯¹é½å¤±è´¥: {e}")
            traceback.print_exc()
            return {
                'success': False,
                'error': str(e),
                'standard_speech_audio': standard_audio,
                'user_speech_audio': user_audio
            }
    
    def _calculate_alignment_quality(self, std_info: Dict, user_info: Dict) -> Dict:
        """è®¡ç®—å¯¹é½è´¨é‡æŒ‡æ ‡"""
        try:
            # è¯­éŸ³æ¯”ä¾‹å·®å¼‚
            ratio_diff = abs(std_info['speech_ratio'] - user_info['speech_ratio'])
            
            # æ®µæ•°å·®å¼‚
            segment_diff = abs(std_info['segment_count'] - user_info['segment_count'])
            
            # æ—¶é•¿å·®å¼‚
            duration_diff = abs(std_info['speech_duration'] - user_info['speech_duration'])
            
            # è®¡ç®—è´¨é‡è¯„åˆ† (0-1ï¼Œè¶Šé«˜è¶Šå¥½)
            ratio_score = max(0, 1 - ratio_diff * 2)  # æ¯”ä¾‹å·®å¼‚æƒé‡
            segment_score = max(0, 1 - segment_diff * 0.1)  # æ®µæ•°å·®å¼‚æƒé‡
            duration_score = max(0, 1 - duration_diff * 0.1)  # æ—¶é•¿å·®å¼‚æƒé‡
            
            overall_score = (ratio_score + segment_score + duration_score) / 3
            
            return {
                'overall_score': overall_score,
                'ratio_difference': ratio_diff,
                'segment_difference': segment_diff,
                'duration_difference': duration_diff,
                'quality_level': 'high' if overall_score > 0.8 else 
                               'medium' if overall_score > 0.6 else 'low'
            }
            
        except Exception as e:
            return {
                'overall_score': 0.5,
                'quality_level': 'unknown',
                'error': str(e)
            }

# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
if __name__ == '__main__':
    import sys
    
    # åˆ›å»ºVADå¤„ç†å™¨
    vad = VADProcessor()
    
    print("=== VADæ¨¡å—çŠ¶æ€ ===")
    print(f"DashScopeå¯ç”¨: {DASHSCOPE_AVAILABLE}")
    print(f"FunASRå¯ç”¨: {FUNASR_AVAILABLE}")
    print(f"æœ¬åœ°VADæ¨¡å‹: {'å¯ç”¨' if vad.vad_available else 'ä¸å¯ç”¨'}")
    
    # å¦‚æœæä¾›äº†æµ‹è¯•éŸ³é¢‘ï¼Œè¿›è¡Œæµ‹è¯•
    if len(sys.argv) > 1:
        test_audio = sys.argv[1]
        if os.path.exists(test_audio):
            print(f"\n=== æµ‹è¯•VAD: {test_audio} ===")
            
            # æ£€æµ‹è¯­éŸ³æ®µ
            segments = vad.detect_speech_segments(test_audio)
            print(f"æ£€æµ‹åˆ°è¯­éŸ³æ®µ: {segments}")
            
            # è·å–è¯¦ç»†ä¿¡æ¯
            info = vad.get_speech_regions_timestamps(test_audio)
            print(f"æ€»æ—¶é•¿: {info['total_duration']:.2f}s")
            print(f"è¯­éŸ³æ—¶é•¿: {info['speech_duration']:.2f}s")
            print(f"è¯­éŸ³æ¯”ä¾‹: {info['speech_ratio']:.2%}")
            
            # æå–çº¯è¯­éŸ³
            speech_path = vad.extract_speech_audio(test_audio)
            print(f"çº¯è¯­éŸ³æ–‡ä»¶: {speech_path}")
        else:
            print(f"æµ‹è¯•éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {test_audio}")
    else:
        print("\næç¤º: å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œæµ‹è¯•:")
        print("python vad_module.py test_audio.wav")
