# 频谱图可视化发音训练系统实现方案

## 一、方案合理性评估

### ✅ 优势与可行性

1. **声学基础扎实**
   - 频谱图（Spectrogram）是语音分析的金标准工具
   - VOT（Voice Onset Time）差异在频谱图上可量化、可视化
   - zhi/chi 等送气/不送气音对的区分度极高（VOT差异可达50-100ms）

2. **教学价值显著**
   - 将抽象的"送气"概念转化为可见的"胖云 vs 瘦条"
   - 实时视觉反馈比单纯听觉反馈更直观
   - 符合多感官学习理论（视觉+听觉+动觉）

3. **技术成熟度高**
   - Python生态完善：librosa、scipy、matplotlib
   - 实时处理可行：pyaudio + 滑动窗口
   - 与现有ASR/TTS模块集成度高

4. **数据价值**
   - 频谱图可直接作为深度学习模型输入（CNN/Transformer）
   - 提供更精准的数据标注依据
   - 增强模型可解释性

### ⚠️ 挑战与限制

1. **认知负荷**
   - 普通用户需要学习如何"读懂"频谱图
   - 需要设计游戏化、简化的交互界面

2. **实时性能**
   - 实时频谱计算和渲染可能有延迟（需优化至<100ms）
   - 移动端性能挑战（H5/移动端已在你们的系统中）

3. **个体差异**
   - 不同人声道结构导致频谱细节差异
   - 需要建立动态基线而非固定模板

**结论：方案整体合理且可行，建议分阶段实施。**

---

## 二、核心技术实现方案

### 2.1 频谱图生成技术栈

#### 基础库选型
```python
# 推荐技术栈
import librosa              # 专业音频分析库
import numpy as np
import matplotlib.pyplot as plt
from scipy import signal
import sounddevice as sd    # 实时音频输入
```

#### 关键参数设置
```python
# 频谱图生成最佳实践参数
SAMPLE_RATE = 16000        # 采样率（16kHz足够语音分析）
N_FFT = 512                # FFT窗口大小（影响频率分辨率）
HOP_LENGTH = 128           # 帧移（影响时间分辨率）
N_MELS = 128               # Mel滤波器数量（可选，用于Mel频谱图）
F_MIN = 80                 # 最低频率（过滤掉低频噪音）
F_MAX = 8000               # 最高频率（语音主要能量区）
```

### 2.2 实时频谱图实现

#### 方法一：完整频谱图（用于录音后分析）

```python
def generate_spectrogram(audio_path, output_path=None):
    """
    生成完整的频谱图用于展示和分析
    
    参数:
        audio_path: 音频文件路径
        output_path: 保存图像路径（可选）
    
    返回:
        spectrogram: 频谱图数据（numpy数组）
        times: 时间轴
        frequencies: 频率轴
    """
    # 加载音频
    y, sr = librosa.load(audio_path, sr=SAMPLE_RATE)
    
    # 生成短时傅里叶变换（STFT）
    D = librosa.stft(y, n_fft=N_FFT, hop_length=HOP_LENGTH)
    
    # 转换为分贝刻度
    S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)
    
    # 时间和频率轴
    times = librosa.frames_to_time(range(S_db.shape[1]), 
                                     sr=sr, hop_length=HOP_LENGTH)
    frequencies = librosa.fft_frequencies(sr=sr, n_fft=N_FFT)
    
    # 可视化
    if output_path:
        plt.figure(figsize=(12, 6))
        librosa.display.specshow(S_db, sr=sr, hop_length=HOP_LENGTH,
                                  x_axis='time', y_axis='hz',
                                  cmap='hot')
        plt.colorbar(format='%+2.0f dB')
        plt.title('语音频谱图')
        plt.xlabel('时间 (秒)')
        plt.ylabel('频率 (Hz)')
        plt.ylim([0, 8000])  # 限制显示范围
        plt.tight_layout()
        plt.savefig(output_path, dpi=150)
        plt.close()
    
    return S_db, times, frequencies
```

#### 方法二：实时流式频谱图（用于录音时实时反馈）

```python
import queue
import threading

class RealtimeSpectrogram:
    """实时频谱图生成器"""
    
    def __init__(self, sample_rate=16000, window_size=1024, 
                 hop_length=512, buffer_seconds=2):
        self.sr = sample_rate
        self.window_size = window_size
        self.hop_length = hop_length
        self.buffer_size = int(buffer_seconds * sample_rate)
        
        # 环形缓冲区
        self.audio_buffer = np.zeros(self.buffer_size)
        self.audio_queue = queue.Queue()
        
    def audio_callback(self, indata, frames, time, status):
        """音频输入回调函数"""
        if status:
            print(f"音频状态: {status}")
        self.audio_queue.put(indata.copy())
    
    def start_stream(self):
        """启动音频流"""
        self.stream = sd.InputStream(
            channels=1,
            samplerate=self.sr,
            callback=self.audio_callback,
            blocksize=self.hop_length
        )
        self.stream.start()
        
    def stop_stream(self):
        """停止音频流"""
        if hasattr(self, 'stream'):
            self.stream.stop()
            self.stream.close()
    
    def get_current_spectrogram(self):
        """获取当前缓冲区的频谱图"""
        # 更新缓冲区
        while not self.audio_queue.empty():
            chunk = self.audio_queue.get()
            # 滚动缓冲区
            self.audio_buffer = np.roll(self.audio_buffer, -len(chunk))
            self.audio_buffer[-len(chunk):] = chunk.flatten()
        
        # 计算频谱图
        f, t, Sxx = signal.spectrogram(
            self.audio_buffer, 
            fs=self.sr,
            nperseg=self.window_size,
            noverlap=self.window_size - self.hop_length
        )
        
        # 转换为dB
        Sxx_db = 10 * np.log10(Sxx + 1e-10)
        
        return f, t, Sxx_db

# 使用示例
realtime_spec = RealtimeSpectrogram()
realtime_spec.start_stream()

# 在主循环中获取频谱图并更新UI
while recording:
    f, t, spec = realtime_spec.get_current_spectrogram()
    # 更新前端显示（通过WebSocket或轮询）
    update_frontend_spectrogram(spec)
    time.sleep(0.05)  # 20 FPS

realtime_spec.stop_stream()
```

### 2.3 关键特征提取

#### VOT（Voice Onset Time）自动检测

```python
def detect_vot(audio_path, threshold_db=-40, min_duration_ms=20):
    """
    自动检测VOT（送气持续时间）
    
    参数:
        audio_path: 音频路径
        threshold_db: 能量阈值（dB）
        min_duration_ms: 最小持续时间（毫秒）
    
    返回:
        vot_ms: VOT时长（毫秒）
        burst_start: 爆破开始时间
        voice_start: 浊音开始时间
    """
    y, sr = librosa.load(audio_path, sr=SAMPLE_RATE)
    
    # 计算短时能量
    frame_length = int(0.01 * sr)  # 10ms帧
    energy = np.array([
        np.sum(y[i:i+frame_length]**2) 
        for i in range(0, len(y)-frame_length, frame_length//2)
    ])
    
    # 转换为dB
    energy_db = 10 * np.log10(energy + 1e-10)
    
    # 找到第一个超过阈值的点（爆破开始）
    burst_frames = np.where(energy_db > threshold_db)[0]
    if len(burst_frames) == 0:
        return None, None, None
    
    burst_start = burst_frames[0]
    
    # 找到能量稳定增长的点（浊音开始）
    # 简单方法：找到能量达到峰值50%的点
    peak_energy = np.max(energy_db[burst_start:])
    voice_threshold = peak_energy - 10  # 峰值以下10dB
    voice_frames = np.where(energy_db[burst_start:] > voice_threshold)[0]
    
    if len(voice_frames) == 0:
        return None, None, None
    
    voice_start = burst_start + voice_frames[0]
    
    # 计算VOT（毫秒）
    vot_frames = voice_start - burst_start
    vot_ms = vot_frames * (frame_length / 2) / sr * 1000
    
    return vot_ms, burst_start, voice_start
```

#### 送气强度量化

```python
def quantify_aspiration(audio_path, vot_region=None):
    """
    量化送气段的强度和特征
    
    返回:
        aspiration_score: 送气强度评分（0-100）
        features: 详细特征字典
    """
    y, sr = librosa.load(audio_path, sr=SAMPLE_RATE)
    
    # 如果没有指定VOT区域，自动检测
    if vot_region is None:
        vot_ms, burst_start, voice_start = detect_vot(audio_path)
        if vot_ms is None:
            return 0, {}
        # 转换为样本索引
        frame_samples = int(0.01 * sr)
        start_sample = burst_start * frame_samples // 2
        end_sample = voice_start * frame_samples // 2
    else:
        start_sample, end_sample = vot_region
    
    # 提取送气段
    aspiration_segment = y[start_sample:end_sample]
    
    # 特征1：持续时间
    duration_ms = len(aspiration_segment) / sr * 1000
    
    # 特征2：平均能量
    energy = np.mean(aspiration_segment**2)
    energy_db = 10 * np.log10(energy + 1e-10)
    
    # 特征3：高频能量占比（送气主要在高频）
    D = librosa.stft(aspiration_segment, n_fft=512)
    power_spectrum = np.abs(D)**2
    freqs = librosa.fft_frequencies(sr=sr, n_fft=512)
    
    # 计算3kHz以上的能量占比
    high_freq_mask = freqs > 3000
    high_freq_ratio = (np.sum(power_spectrum[high_freq_mask, :]) / 
                       np.sum(power_spectrum))
    
    # 特征4：频谱平坦度（噪音特征）
    spectral_flatness = np.mean(librosa.feature.spectral_flatness(y=aspiration_segment))
    
    # 综合评分（经验公式）
    duration_score = min(duration_ms / 100 * 100, 100)  # 100ms为满分
    energy_score = min((energy_db + 60) / 30 * 100, 100)  # -60到-30dB映射到0-100
    high_freq_score = high_freq_ratio * 100
    flatness_score = spectral_flatness[0] * 100
    
    aspiration_score = (
        duration_score * 0.4 + 
        energy_score * 0.3 + 
        high_freq_score * 0.2 + 
        flatness_score * 0.1
    )
    
    features = {
        'duration_ms': duration_ms,
        'energy_db': energy_db,
        'high_freq_ratio': high_freq_ratio,
        'spectral_flatness': spectral_flatness[0],
        'aspiration_score': aspiration_score
    }
    
    return aspiration_score, features
```

### 2.4 zhi/chi 分类器

```python
def classify_zhi_chi(audio_path):
    """
    自动分类 zhi（不送气）vs chi（送气）
    
    返回:
        prediction: 'zhi' 或 'chi'
        confidence: 置信度（0-1）
        details: 详细分析结果
    """
    # 检测VOT
    vot_ms, burst_start, voice_start = detect_vot(audio_path)
    
    if vot_ms is None:
        return None, 0, {'error': '无法检测到有效音频'}
    
    # 量化送气强度
    y, sr = librosa.load(audio_path, sr=SAMPLE_RATE)
    frame_samples = int(0.01 * sr)
    start_sample = burst_start * frame_samples // 2
    end_sample = voice_start * frame_samples // 2
    
    aspiration_score, features = quantify_aspiration(
        audio_path, 
        vot_region=(start_sample, end_sample)
    )
    
    # 分类规则（基于标准普通话数据）
    # zhi: VOT < 30ms, 送气强度 < 40
    # chi: VOT > 60ms, 送气强度 > 60
    
    zhi_score = 0
    chi_score = 0
    
    # VOT维度
    if vot_ms < 30:
        zhi_score += 3
    elif vot_ms > 60:
        chi_score += 3
    else:
        # 中间区域
        ratio = (vot_ms - 30) / 30
        chi_score += ratio * 2
        zhi_score += (1 - ratio) * 2
    
    # 送气强度维度
    if aspiration_score < 40:
        zhi_score += 2
    elif aspiration_score > 60:
        chi_score += 2
    else:
        ratio = (aspiration_score - 40) / 20
        chi_score += ratio
        zhi_score += (1 - ratio)
    
    # 高频能量维度
    if features['high_freq_ratio'] > 0.3:
        chi_score += 1
    else:
        zhi_score += 1
    
    # 归一化置信度
    total = zhi_score + chi_score
    zhi_confidence = zhi_score / total
    chi_confidence = chi_score / total
    
    if chi_confidence > zhi_confidence:
        prediction = 'chi'
        confidence = chi_confidence
    else:
        prediction = 'zhi'
        confidence = zhi_confidence
    
    details = {
        'vot_ms': vot_ms,
        'aspiration_score': aspiration_score,
        'features': features,
        'zhi_confidence': zhi_confidence,
        'chi_confidence': chi_confidence
    }
    
    return prediction, confidence, details
```

---

## 三、前端可视化实现

### 3.1 Web端实时显示（推荐Canvas）

```javascript
// static/js/spectrogram-visualizer.js

class SpectrogramVisualizer {
    constructor(canvasId, options = {}) {
        this.canvas = document.getElementById(canvasId);
        this.ctx = this.canvas.getContext('2d');
        
        // 配置参数
        this.options = {
            width: options.width || 800,
            height: options.height || 400,
            colormap: options.colormap || 'hot', // 'hot', 'viridis', 'gray'
            minDb: options.minDb || -80,
            maxDb: options.maxDb || 0,
            showGrid: options.showGrid !== false,
            highlightVOT: options.highlightVOT !== false
        };
        
        this.canvas.width = this.options.width;
        this.canvas.height = this.options.height;
        
        // 色图映射
        this.setupColormap();
    }
    
    setupColormap() {
        // 'hot' 色图：黑->红->黄->白
        this.colormap = {
            hot: (value) => {
                // value: 0-1
                const r = Math.min(255, value * 3 * 255);
                const g = Math.min(255, Math.max(0, (value - 0.33) * 3 * 255));
                const b = Math.min(255, Math.max(0, (value - 0.66) * 3 * 255));
                return `rgb(${r}, ${g}, ${b})`;
            },
            viridis: (value) => {
                // 简化版 viridis
                const colors = [
                    [68, 1, 84],     // 深紫
                    [59, 82, 139],   // 蓝
                    [33, 145, 140],  // 青
                    [94, 201, 98],   // 绿
                    [253, 231, 37]   // 黄
                ];
                const idx = value * (colors.length - 1);
                const i = Math.floor(idx);
                const j = Math.min(i + 1, colors.length - 1);
                const t = idx - i;
                
                const r = colors[i][0] + (colors[j][0] - colors[i][0]) * t;
                const g = colors[i][1] + (colors[j][1] - colors[i][1]) * t;
                const b = colors[i][2] + (colors[j][2] - colors[i][2]) * t;
                
                return `rgb(${r}, ${g}, ${b})`;
            }
        };
    }
    
    draw(spectrogramData) {
        /**
         * spectrogramData: {
         *   spec: 2D array [freq_bins, time_frames],
         *   times: 1D array,
         *   frequencies: 1D array,
         *   vot_regions: [{start, end, type: 'zhi'/'chi'}]  // 可选
         * }
         */
        const { spec, times, frequencies, vot_regions } = spectrogramData;
        
        // 清空画布
        this.ctx.fillStyle = 'black';
        this.ctx.fillRect(0, 0, this.canvas.width, this.canvas.height);
        
        // 计算像素映射
        const freqBins = spec.length;
        const timeFrames = spec[0].length;
        const pixelWidth = this.canvas.width / timeFrames;
        const pixelHeight = this.canvas.height / freqBins;
        
        // 绘制频谱图
        const getColor = this.colormap[this.options.colormap] || this.colormap.hot;
        
        for (let f = 0; f < freqBins; f++) {
            for (let t = 0; t < timeFrames; t++) {
                // 归一化到 0-1
                const db = spec[f][t];
                const normalized = (db - this.options.minDb) / 
                                   (this.options.maxDb - this.options.minDb);
                const value = Math.max(0, Math.min(1, normalized));
                
                // 设置颜色
                this.ctx.fillStyle = getColor(value);
                
                // 绘制像素（注意Y轴翻转）
                const x = t * pixelWidth;
                const y = this.canvas.height - (f + 1) * pixelHeight;
                this.ctx.fillRect(x, y, pixelWidth + 1, pixelHeight + 1);
            }
        }
        
        // 高亮VOT区域
        if (this.options.highlightVOT && vot_regions) {
            vot_regions.forEach(region => {
                const startX = (region.start / times[times.length - 1]) * 
                               this.canvas.width;
                const endX = (region.end / times[times.length - 1]) * 
                             this.canvas.width;
                const width = endX - startX;
                
                // 绘制半透明矩形
                this.ctx.fillStyle = region.type === 'chi' ? 
                    'rgba(255, 0, 0, 0.2)' : 'rgba(0, 255, 0, 0.2)';
                this.ctx.fillRect(startX, 0, width, this.canvas.height);
                
                // 绘制边框
                this.ctx.strokeStyle = region.type === 'chi' ? 
                    'rgba(255, 0, 0, 0.8)' : 'rgba(0, 255, 0, 0.8)';
                this.ctx.lineWidth = 2;
                this.ctx.strokeRect(startX, 0, width, this.canvas.height);
                
                // 添加标签
                this.ctx.fillStyle = 'white';
                this.ctx.font = 'bold 16px sans-serif';
                this.ctx.fillText(
                    region.type.toUpperCase(), 
                    startX + 5, 
                    20
                );
            });
        }
        
        // 绘制坐标轴和网格
        if (this.options.showGrid) {
            this.drawAxes(times, frequencies);
        }
    }
    
    drawAxes(times, frequencies) {
        this.ctx.strokeStyle = 'rgba(255, 255, 255, 0.3)';
        this.ctx.lineWidth = 1;
        this.ctx.font = '12px sans-serif';
        this.ctx.fillStyle = 'white';
        
        // 时间轴（X轴）- 绘制5条垂直线
        for (let i = 0; i <= 5; i++) {
            const x = (i / 5) * this.canvas.width;
            const time = (i / 5) * times[times.length - 1];
            
            this.ctx.beginPath();
            this.ctx.moveTo(x, 0);
            this.ctx.lineTo(x, this.canvas.height);
            this.ctx.stroke();
            
            this.ctx.fillText(
                `${time.toFixed(2)}s`, 
                x + 2, 
                this.canvas.height - 5
            );
        }
        
        // 频率轴（Y轴）- 绘制5条水平线
        const maxFreq = frequencies[frequencies.length - 1];
        for (let i = 0; i <= 5; i++) {
            const y = (i / 5) * this.canvas.height;
            const freq = ((5 - i) / 5) * maxFreq;
            
            this.ctx.beginPath();
            this.ctx.moveTo(0, y);
            this.ctx.lineTo(this.canvas.width, y);
            this.ctx.stroke();
            
            this.ctx.fillText(
                `${Math.round(freq)}Hz`, 
                5, 
                y - 2
            );
        }
    }
    
    // 添加"目标模板"叠加层
    drawTemplate(templateType) {
        // templateType: 'zhi' 或 'chi'
        this.ctx.strokeStyle = 'rgba(0, 255, 255, 0.8)';
        this.ctx.lineWidth = 3;
        this.ctx.setLineDash([5, 5]);
        
        if (templateType === 'zhi') {
            // 绘制"瘦条"引导线
            const x = this.canvas.width * 0.1;
            const width = this.canvas.width * 0.05;  // 窄
            this.ctx.strokeRect(x, 0, width, this.canvas.height * 0.6);
            
            // 提示文字
            this.ctx.fillStyle = 'cyan';
            this.ctx.font = 'bold 20px sans-serif';
            this.ctx.fillText('目标：短促的瘦条', x, this.canvas.height - 20);
        } else if (templateType === 'chi') {
            // 绘制"胖云"引导线
            const x = this.canvas.width * 0.1;
            const width = this.canvas.width * 0.15;  // 宽
            this.ctx.strokeRect(x, 0, width, this.canvas.height * 0.8);
            
            this.ctx.fillStyle = 'cyan';
            this.ctx.font = 'bold 20px sans-serif';
            this.ctx.fillText('目标：长而密集的胖云', x, this.canvas.height - 20);
        }
        
        this.ctx.setLineDash([]);  // 恢复实线
    }
}

// 使用示例
const visualizer = new SpectrogramVisualizer('spectrogram-canvas', {
    width: 1000,
    height: 500,
    colormap: 'hot',
    highlightVOT: true
});

// 从后端获取数据并显示
async function updateSpectrogram() {
    const response = await fetch('/api/get_realtime_spectrogram');
    const data = await response.json();
    visualizer.draw(data);
}

// 20 FPS 更新
setInterval(updateSpectrogram, 50);
```

### 3.2 Flask后端API

```python
# web_interface.py 中添加

from flask import jsonify
import json
import base64
from io import BytesIO

@app.route('/api/generate_spectrogram', methods=['POST'])
def api_generate_spectrogram():
    """生成完整频谱图（录音结束后）"""
    try:
        audio_file = request.files.get('audio')
        if not audio_file:
            return jsonify({'error': '未上传音频文件'}), 400
        
        # 保存临时文件
        temp_path = os.path.join('temp', f'temp_{int(time.time())}.wav')
        audio_file.save(temp_path)
        
        # 生成频谱图
        S_db, times, frequencies = generate_spectrogram(temp_path)
        
        # 检测VOT和分类
        prediction, confidence, details = classify_zhi_chi(temp_path)
        
        # 清理临时文件
        os.remove(temp_path)
        
        # 返回数据
        return jsonify({
            'success': True,
            'spectrogram': S_db.tolist(),
            'times': times.tolist(),
            'frequencies': frequencies.tolist(),
            'prediction': prediction,
            'confidence': float(confidence),
            'details': {
                'vot_ms': float(details['vot_ms']),
                'aspiration_score': float(details['aspiration_score']),
                'zhi_confidence': float(details['zhi_confidence']),
                'chi_confidence': float(details['chi_confidence'])
            }
        })
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500


# 实时频谱图（使用WebSocket或SSE）
from flask_socketio import SocketIO, emit

socketio = SocketIO(app)

# 全局实时频谱生成器
realtime_spectrograms = {}

@socketio.on('start_recording')
def handle_start_recording(data):
    session_id = request.sid
    realtime_spectrograms[session_id] = RealtimeSpectrogram()
    realtime_spectrograms[session_id].start_stream()
    emit('recording_started', {'status': 'success'})

@socketio.on('stop_recording')
def handle_stop_recording():
    session_id = request.sid
    if session_id in realtime_spectrograms:
        realtime_spectrograms[session_id].stop_stream()
        del realtime_spectrograms[session_id]
    emit('recording_stopped', {'status': 'success'})

@socketio.on('request_spectrogram')
def handle_request_spectrogram():
    """客户端请求当前频谱图"""
    session_id = request.sid
    if session_id in realtime_spectrograms:
        f, t, spec = realtime_spectrograms[session_id].get_current_spectrogram()
        
        emit('spectrogram_update', {
            'frequencies': f.tolist(),
            'times': t.tolist(),
            'spec': spec.tolist()
        })
```

---

## 四、游戏化教学模块设计

### 4.1 "频谱镜子"模式

#### 功能描述
用户实时看到自己的发音频谱图，系统叠加"理想模板"作为目标。

#### 交互流程
1. 用户选择练习音节（zhi/chi）
2. 系统显示目标频谱模板（半透明叠加）
3. 用户开始发音，实时频谱图同步显示
4. 系统实时给出相似度评分（颜色变化：红→黄→绿）
5. 录音结束后，显示详细分析和改进建议

#### 实现要点
```python
def calculate_spectrogram_similarity(user_spec, template_spec):
    """
    计算用户频谱图与模板的相似度
    
    使用结构相似性指数（SSIM）或余弦相似度
    """
    from skimage.metrics import structural_similarity as ssim
    
    # 调整尺寸一致
    if user_spec.shape != template_spec.shape:
        from scipy.ndimage import zoom
        zoom_factors = (template_spec.shape[0] / user_spec.shape[0],
                       template_spec.shape[1] / user_spec.shape[1])
        user_spec = zoom(user_spec, zoom_factors)
    
    # 归一化
    user_norm = (user_spec - user_spec.min()) / (user_spec.max() - user_spec.min())
    template_norm = (template_spec - template_spec.min()) / (template_spec.max() - template_spec.min())
    
    # 计算SSIM
    similarity = ssim(user_norm, template_norm)
    
    return similarity * 100  # 转换为0-100分
```

### 4.2 "对比游戏"模式

#### 功能描述
用户连续发"zhi-chi-zhi-chi"，系统在一个画面上并排显示所有频谱图，强化对比。

#### 示例界面
```
┌─────────────────────────────────────────────┐
│  你的发音                                      │
│  ┌─────┐  ┌──────┐  ┌─────┐  ┌──────┐     │
│  │ 瘦条 │  │ 胖云？│  │ 瘦条 │  │  胖云 │     │
│  └─────┘  └──────┘  └─────┘  └──────┘     │
│   zhi      chi       zhi       chi         │
│   ✓        ✗         ✓         ✓          │
│                                             │
│  问题：第2次"chi"送气不足！                   │
│  建议：用力吹气，让VOT超过60ms                │
└─────────────────────────────────────────────┘
```

### 4.3 "挑战关卡"模式

#### 关卡设计
- **关卡1**：单音节识别（准确度 >70%）
- **关卡2**：快速切换（zhi-chi-zhi 间隔<1秒）
- **关卡3**：多音节词汇（"吃纸、知识、迟疑"）
- **关卡4**：句子（"这只蜘蛛吃纸"）
- **关卡5**：自由对话（AI评分）

#### 评分维度
```python
def calculate_game_score(user_audio, target_phoneme):
    """综合评分"""
    scores = {}
    
    # 1. 分类准确性（40分）
    prediction, confidence, details = classify_zhi_chi(user_audio)
    scores['classification'] = 40 if prediction == target_phoneme else 0
    
    # 2. VOT准确性（30分）
    target_vot = 20 if target_phoneme == 'zhi' else 80
    vot_error = abs(details['vot_ms'] - target_vot)
    scores['vot'] = max(0, 30 - vot_error / 2)  # 每偏离2ms扣1分
    
    # 3. 送气强度（20分）
    target_aspiration = 30 if target_phoneme == 'zhi' else 75
    asp_error = abs(details['aspiration_score'] - target_aspiration)
    scores['aspiration'] = max(0, 20 - asp_error / 5)
    
    # 4. 稳定性（10分）
    # 需要多次发音的标准差
    scores['stability'] = 10  # 简化版
    
    total = sum(scores.values())
    
    return {
        'total': total,
        'breakdown': scores,
        'grade': 'S' if total >= 90 else 
                 'A' if total >= 80 else 
                 'B' if total >= 70 else 'C'
    }
```

---

## 五、与现有系统集成

### 5.1 集成到现有页面

在 `templates/hearing_feedback.html` 中添加频谱图标签：

```html
<!-- 在音高曲线图下方添加 -->
<div class="spectrogram-container">
    <h3>实时频谱图</h3>
    <canvas id="spectrogram-canvas" width="1000" height="400"></canvas>
    
    <div class="spectrogram-controls">
        <label>
            <input type="checkbox" id="show-template" />
            显示目标模板
        </label>
        <select id="template-type">
            <option value="">无</option>
            <option value="zhi">zhi（不送气）</option>
            <option value="chi">chi（送气）</option>
        </select>
    </div>
    
    <div class="analysis-results" id="vot-analysis">
        <h4>发音分析</h4>
        <div class="metric">
            <span class="label">VOT:</span>
            <span class="value" id="vot-value">--</span> ms
        </div>
        <div class="metric">
            <span class="label">送气强度:</span>
            <span class="value" id="aspiration-value">--</span> / 100
        </div>
        <div class="metric">
            <span class="label">判定结果:</span>
            <span class="value" id="classification-result">--</span>
        </div>
    </div>
</div>
```

### 5.2 与ASR模块结合

修改 `fun_asr_module.py` 或创建新的 `spectral_asr_module.py`：

```python
class SpectralASR:
    """基于频谱特征的增强ASR"""
    
    def __init__(self, base_asr_model):
        self.base_asr = base_asr_model
        self.phoneme_classifier = self.load_phoneme_classifier()
    
    def recognize_with_spectral_features(self, audio_path):
        """结合传统ASR和频谱特征的识别"""
        # 1. 基础ASR识别
        base_result = self.base_asr.recognize(audio_path)
        
        # 2. 频谱特征分析
        prediction, confidence, details = classify_zhi_chi(audio_path)
        
        # 3. 如果基础ASR在 zhi/chi 上不确定，使用频谱分类器
        if self.contains_ambiguous_phonemes(base_result):
            # 替换模糊音素
            corrected_result = self.correct_with_spectral(
                base_result, 
                prediction, 
                confidence
            )
            return corrected_result
        
        return base_result
    
    def contains_ambiguous_phonemes(self, asr_result):
        """检查是否包含容易混淆的音素"""
        ambiguous = ['zhi', 'chi', 'shi', 'zi', 'ci', 'si']
        return any(ph in asr_result['text'] for ph in ambiguous)
```

### 5.3 数据收集与标注

创建 `spectral_data_collector.py`：

```python
class SpectralDataCollector:
    """收集带频谱标注的训练数据"""
    
    def __init__(self, output_dir='data/spectral_dataset'):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
    
    def collect_sample(self, audio_path, ground_truth_label, user_id=None):
        """收集单个样本"""
        # 生成频谱图
        S_db, times, frequencies = generate_spectrogram(audio_path)
        
        # 提取特征
        vot_ms, burst_start, voice_start = detect_vot(audio_path)
        aspiration_score, features = quantify_aspiration(audio_path)
        
        # 构建样本数据
        sample = {
            'audio_path': audio_path,
            'spectrogram': S_db.tolist(),
            'label': ground_truth_label,
            'features': {
                'vot_ms': vot_ms,
                'aspiration_score': aspiration_score,
                **features
            },
            'user_id': user_id,
            'timestamp': time.time()
        }
        
        # 保存
        sample_id = hashlib.md5(
            f"{audio_path}{time.time()}".encode()
        ).hexdigest()[:12]
        
        output_file = os.path.join(
            self.output_dir, 
            f"{ground_truth_label}_{sample_id}.json"
        )
        
        with open(output_file, 'w') as f:
            json.dump(sample, f, indent=2)
        
        # 同时保存音频副本
        shutil.copy(
            audio_path, 
            os.path.join(self.output_dir, f"{sample_id}.wav")
        )
        
        return sample_id
```

---

## 六、性能优化建议

### 6.1 实时处理优化

```python
# 使用GPU加速（如果可用）
import torch

def fast_spectrogram_gpu(audio_signal, sr=16000):
    """GPU加速的频谱图计算"""
    if torch.cuda.is_available():
        # 转换为Torch张量
        audio_tensor = torch.from_numpy(audio_signal).cuda()
        
        # 使用torchaudio
        import torchaudio
        spectrogram_transform = torchaudio.transforms.Spectrogram(
            n_fft=512,
            hop_length=128
        ).cuda()
        
        spec = spectrogram_transform(audio_tensor)
        return spec.cpu().numpy()
    else:
        # 回退到CPU
        return librosa.stft(audio_signal)
```

### 6.2 Web端性能优化

```javascript
// 使用Web Worker处理数据
// spectrogram-worker.js
self.onmessage = function(e) {
    const { spec, colormap, minDb, maxDb } = e.data;
    
    // 在worker中进行数据处理和颜色映射
    const imageData = processSpectrogram(spec, colormap, minDb, maxDb);
    
    self.postMessage({ imageData });
};

// 主线程
const worker = new Worker('spectrogram-worker.js');
worker.postMessage({ spec, colormap, minDb, maxDb });
worker.onmessage = (e) => {
    // 直接绘制处理好的ImageData
    ctx.putImageData(e.data.imageData, 0, 0);
};
```

### 6.3 移动端适配

```css
/* static/css/spectrogram.css */
.spectrogram-container {
    width: 100%;
    max-width: 1000px;
    margin: 20px auto;
    padding: 15px;
    background: #1a1a1a;
    border-radius: 8px;
}

#spectrogram-canvas {
    width: 100%;
    height: auto;
    max-height: 400px;
    border: 1px solid #444;
    border-radius: 4px;
}

/* 移动端优化 */
@media (max-width: 768px) {
    #spectrogram-canvas {
        max-height: 250px;
    }
    
    .analysis-results {
        font-size: 14px;
    }
}
```

---

## 七、实施路线图

### 阶段1：核心功能开发（1-2周）
- [ ] 实现频谱图生成函数（离线版）
- [ ] 实现VOT检测算法
- [ ] 实现zhi/chi分类器
- [ ] 创建基础Web可视化界面

### 阶段2：实时功能（1周）
- [ ] 实现实时音频流处理
- [ ] 集成WebSocket实时通信
- [ ] 优化前端Canvas渲染性能

### 阶段3：游戏化功能（1-2周）
- [ ] 开发"频谱镜子"模式
- [ ] 开发"对比游戏"模式
- [ ] 设计挑战关卡系统
- [ ] 实现评分和反馈机制

### 阶段4：系统集成（1周）
- [ ] 与现有听觉反馈系统集成
- [ ] 与ASR模块结合
- [ ] 数据收集和标注工具
- [ ] 移动端适配和测试

### 阶段5：优化与迭代（持续）
- [ ] 用户测试和反馈收集
- [ ] 性能优化（GPU加速、压缩传输）
- [ ] 扩展到更多音素对（zh/ch, j/q, z/c等）
- [ ] 机器学习模型训练和部署

---

## 八、预期效果与评估指标

### 用户侧指标
- **学习曲线**：用户能在5次练习内显著提高准确率
- **用户满意度**：目标4.5+/5.0（问卷调查）
- **留存率**：7日留存率 >60%

### 技术侧指标
- **分类准确率**：zhi/chi自动分类准确率 >90%
- **实时延迟**：频谱图更新延迟 <100ms
- **系统性能**：支持50+并发用户

### 教学效果指标
- **前后测对比**：使用前后发音准确率提升 >30%
- **自信度提升**：用户自评发音信心提高 >40%

---

## 九、技术风险与应对

| 风险 | 影响 | 应对策略 |
|------|------|----------|
| 实时计算延迟过大 | 用户体验差 | 降低频谱分辨率、使用GPU加速、优化算法 |
| 移动端性能不足 | 功能受限 | 提供云端计算选项、简化可视化 |
| 个体差异导致误判 | 准确率下降 | 个性化基线校准、自适应阈值 |
| 用户理解困难 | 教学效果打折 | 简化界面、增强引导、游戏化降低门槛 |
| 数据标注成本高 | 模型优化慢 | 半监督学习、主动学习策略 |

---

## 十、参考资源

### 学术基础
1. **VOT研究**：Lisker & Abramson (1964) - 经典VOT分类研究
2. **汉语语音学**：《汉语语音学》（林焘、王理嘉） - 第4章 声母
3. **频谱分析**：*Speech and Language Processing* (Jurafsky & Martin) - Ch.16

### 技术文档
- [Librosa Documentation](https://librosa.org/doc/latest/)
- [Mozilla Web Audio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API)
- [Flask-SocketIO](https://flask-socketio.readthedocs.io/)

### 开源项目参考
- [Praat](https://www.fon.hum.uva.nl/praat/) - 专业语音分析软件
- [SpeechBrain](https://speechbrain.github.io/) - 端到端语音工具包
- [Wavesurfer.js](https://wavesurfer-js.org/) - Web音频波形可视化

---

## 附录：快速开始代码

### 最小可运行示例

```python
# demo_spectrogram.py
import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np

def quick_demo(audio_path):
    """5行代码生成频谱图"""
    y, sr = librosa.load(audio_path, sr=16000)
    D = librosa.stft(y, n_fft=512, hop_length=128)
    S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)
    
    plt.figure(figsize=(10, 4))
    librosa.display.specshow(S_db, sr=sr, hop_length=128,
                              x_axis='time', y_axis='hz', cmap='hot')
    plt.colorbar(format='%+2.0f dB')
    plt.title('频谱图')
    plt.tight_layout()
    plt.savefig('spectrogram_demo.png', dpi=150)
    plt.show()

if __name__ == '__main__':
    # 测试
    quick_demo('test_audio.wav')
```

运行：
```bash
pip install librosa matplotlib
python demo_spectrogram.py
```

---

**文档版本**：v1.0  
**创建时间**：2025-10-15  
**负责人**：音高曲线比对系统开发团队  
**状态**：待实施


